---
title: "R Notebook"
output: html_notebook
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_knit$set(root.dir = here::here("~/GLAMR"))
library(tidyverse)
library(googlesheets4)
library(googledrive)
library(DBI)

conflicted::conflicts_prefer(dplyr::select(), dplyr::rename(), dplyr::filter(), dplyr::lag(), base::as.data.frame, readr::as.col_spec())

# Open database connection
pg <- DBI::dbConnect(RPostgres::Postgres(),dbname = "glamr_data", host = "cayman.earth.lsa.umich.edu", port = "5432", user = "glamr_admin", password = "glamr2023")


# Function for getting sample lists from tables quickly using existing index
get_pg_table_samples <- function(db, table, sampleid_col){
  # Makes use of the SQL index, *much* faster to run
  query <- sql(str_glue("WITH RECURSIVE t AS (
     SELECT min(\"{sampleid_col}\") AS sample FROM \"{table}\"
     UNION ALL
     SELECT (SELECT min(\"{sampleid_col}\") FROM \"{table}\" WHERE \"{sampleid_col}\" > t.sample)
     FROM t WHERE t.sample IS NOT NULL
     )
  SELECT sample FROM t WHERE sample IS NOT NULL
  UNION ALL
  SELECT null WHERE EXISTS(SELECT 1 FROM \"{table}\" WHERE \"{sampleid_col}\" IS NULL);"))
  
  mapped_samples <- DBI::dbGetQuery(db, query)
  return(mapped_samples)
}
```

#### Load sample data

Get the latest table from google drive
```{r}
googledrive::drive_deauth()
googledrive::drive_auth(path = "~/GLAMR/.secrets/glamr-425619-f6508150aa53.json")

as_id("https://docs.google.com/spreadsheets/d/1z2IajV0Ay1lRjH9d0ibNBuf8PY0IbtEz/edit#gid=349037648") %>% 
  drive_download("import/Great_Lakes_Omics_Datasets.xlsx",overwrite = TRUE)
```


```{r}
biosamples <- readxl::read_excel("import/Great_Lakes_Omics_Datasets.xlsx",sheet = "samples",guess_max = 3000)
seqsamples <- readxl::read_excel("import/Great_Lakes_Omics_Datasets.xlsx",sheet = "sequencing",guess_max = 3000)
studies <- readxl::read_excel("import/Great_Lakes_Omics_Datasets.xlsx",sheet = "studiesdatasets",range = "A1:V1000")
coassemblies <- readxl::read_excel("import/Great_Lakes_Omics_Datasets.xlsx",sheet = "coassembly_groups",guess_max = 3000)
defined_sites <- readxl::read_excel("import/Great_Lakes_Omics_Datasets.xlsx",sheet = "sampling_sites") %>% filter(!is.na(lat))
units_and_notes <- readxl::read_excel("import/Great_Lakes_Omics_Datasets.xlsx",sheet = "metadata_units_and_notes")

# Merge biosamples and seqsamples table 
samples <- seqsamples %>% 
  select(SampleID = SeqSampleID,BioSampleID,sample_type,`SRA accession`,`NCBI Biosample`,GOLD_analysis_projectID,GOLD_sequencing_projectID,sequencing_type,amplicon_target,F_primer,R_primer) %>% 
  left_join(biosamples %>% rename(BioSampleID = SampleID)) %>% 
  mutate(sample = SampleID) %>% 
  relocate(SampleID,sample, BioSampleID, StudyID, sample_type, collection_date, lat, lon) %>% 
  filter(!is.na(SampleID))

```

#### Write sample information to postgres database
```{r}
# Clean up sample table from excel 
cleaned_up_sample_table <- samples %>% 
  mutate(date = lubridate::ym(collection_date),
         date = if_else(is.na(date), lubridate::ymd(collection_date), date),
         date = if_else(is.na(date), lubridate::ymd_hms(collection_date), date),
         date = if_else(is.na(date), lubridate::ymd_hm(collection_date), date),
         # date = lubridate::ymd(collection_date),
         # date = if_else(is.na(date), lubridate::ymd_hms(collection_date), date),
         # date = if_else(is.na(date), lubridate::ymd_hm(collection_date), date),
         date_same_year = lubridate::`year<-`(date,2000),
         year = lubridate::year(date),
         across(everything(), ~if_else(.x %in% c("NA", "NF"), NA, .x))) %>% 
  type_convert() %>% 
  mutate(across(c(part_microcyst,lon,nitrate,diss_microcyst, soluble_react_phosp, ammonia, urea, pH, tot_phos,wave_height, wind_speed, salinity, atmospheric_temp, particulate_cyl), ~as.numeric(.x))) %>% 
  filter(!is.na(SampleID))

# Add GVHD samples 
googlesheets4:::gs4_auth(path = "~/GLAMR/.secrets/glamr-425619-f6508150aa53.json")
gvhd_samples <- googlesheets4::read_sheet("https://docs.google.com/spreadsheets/d/1jZLyocWDgjRKwjv0JgBQscLtgD2rIjkApoI2mjLGGqw/edit?gid=0#gid=0")

# Add coassemblies 
coassemblies_clean <- coassemblies %>% 
  mutate(sample_type = "metagenome") %>% 
  select(SampleID = "group_id",description,sample_type, coassembly_samples = "samples", notes)

combined_sample_table <- bind_rows(cleaned_up_sample_table, gvhd_samples,coassemblies_clean) %>% mutate(sample = SampleID) %>% relocate(sample)

# Write table to database
sql('DROP VIEW public.bin_summary') %>% DBI::dbSendQuery(pg, .) # have to drop dependencies first
dbWriteTable(pg,"glamr_samples", combined_sample_table,overwrite = TRUE)
studies %>% 
  filter(!is.na(dataset)) %>% 
  dbWriteTable(pg,"glamr_datasets", value = .,overwrite = TRUE)
```


# Create bin summary view table
```{r}
checkm_pg <- tbl(pg, "checkm")
gtdb_pg <- tbl(pg, "GTDB")
drep_pg <- tbl(pg, "drep")
sampinfo_pg <- tbl(pg, "glamr_samples")


bin_info_view <- checkm_pg %>% 
  select(sample, bin, Completeness, Contamination, `Strain heterogeneity`) %>% 
  #mutate(SampleID = str_remove(bin, "[A-z].*_d\\+$")) %>% 
  left_join(sampinfo_pg %>% select(sample = "SampleID", StudyID)) %>% 
  left_join(gtdb_pg %>% select(sample, bin, classification, domain, phylum, class, order, family, genus, species, red_value)) %>% 
  left_join(drep_pg %>% select(sample, bin, secondary_cluster, is_cluster_rep)) %>% 
  show_query()

sql_query <- glue::glue_sql("
CREATE VIEW bin_summary AS
WITH extracted AS (
  SELECT
    \"bin\",
    \"Completeness\",
    \"Contamination\",
    \"Strain heterogeneity\",
    (regexp_matches(\"bin\", '^(.+_\\d+)_(.+?)_(\\d+)$'))[1] AS \"sample\",
    (regexp_matches(\"bin\", '^(.+_\\d+)_(.+?)_(\\d+)$'))[2] AS \"binner\",
    (regexp_matches(\"bin\", '^(.+_\\d+)_(.+?)_(\\d+)$'))[3] AS \"bin_number\"
  FROM \"checkm\")
SELECT
  extracted.\"bin\",
  extracted.\"sample\",
  \"glamr_samples\".\"StudyID\",
  extracted.\"binner\",
  extracted.\"bin_number\",
  extracted.\"Completeness\",
  extracted.\"Contamination\",
  extracted.\"Strain heterogeneity\",
  \"GTDB\".\"classification\",
  \"GTDB\".\"domain\",
  \"GTDB\".\"phylum\",
  \"GTDB\".\"class\",
  \"GTDB\".\"order\",
  \"GTDB\".\"family\",
  \"GTDB\".\"genus\",
  \"GTDB\".\"species\",
  \"GTDB\".\"red_value\",
  \"drep\".\"secondary_cluster\",
  \"drep\".\"is_cluster_rep\"
FROM extracted
LEFT JOIN \"glamr_samples\"
  ON extracted.\"sample\" = \"glamr_samples\".\"SampleID\"
LEFT JOIN \"GTDB\"
  ON extracted.\"bin\" = \"GTDB\".\"bin\"
LEFT JOIN \"drep\"
  ON extracted.\"bin\" = \"drep\".\"bin\";", .con = pg)

DBI::dbSendQuery(pg, sql_query)
```


#### Load taxonomic information
Output of: 
  taxonkit list --ids 1 -n > references/taxonkit/taxon_names.txt
  taxonkit list --ids 1 | taxonkit lineage | taxonkit reformat -a -i 2 -P > references/taxonkit/standardized_lineages.txt


```{r}
tax_cols <- c("tax_id", "full_lineage", "rank", "std_lineage")
taxa_list_std <- read_tsv("~/references/taxonkit/standardized_lineages_with_rank.txt",col_names = tax_cols)

tax_info <- taxa_list_std %>% 
  separate(std_lineage, into = c("kingdom", "phylum", "class", "order", "family", "genus", "species"),sep = ";[a-z]__" ,remove = FALSE) %>% 
  mutate(tax_name = str_remove(full_lineage, ".*;"),
         kingdom = str_remove(kingdom, "^k__")) %>% 
  relocate(tax_id, tax_name, rank, full_lineage, std_lineage)

dbWriteTable(pg,"tax_info", tax_info, overwrite = TRUE)
sql('CREATE INDEX "tax_info_tax_id" ON "tax_info" ("tax_id")') %>% DBI::dbSendQuery(pg, .)
sql('CREATE INDEX "tax_info_tax_name" ON "tax_info" ("tax_name")') %>% DBI::dbSendQuery(pg, .)
sql('CREATE INDEX "tax_info_rank" ON "tax_info" ("rank")') %>% DBI::dbSendQuery(pg, .)
```


#### Load UniRef db information from mmseqs db
Read in file that maps rows in MMSeqs database to their UniRef100 ids
```{r}
uniref100_lookup <- data.table::fread("data/reference/mmseqs2/uniref100.lookup", col.names = c("id", "uniref100", "extra"),
                                      nThread = 8,showProgress = TRUE)

dbWriteTable(pg,"uniref100_ids_from_mmseqs_db", uniref100_lookup, overwrite = TRUE,field.types = c(id = "integer",uniref100 = "text", extra = "integer"))

rm(uniref100_lookup)
gc()
```

Read in MMseqs database index, which includes the sequence length (the actual length is two less than reported, because the length here also includes a the null byte seperator and the new line charachter)
```{r}
uniref100_index <- data.table::fread("data/reference/mmseqs2/uniref100.index", col.names = c("id", "offset", "length"),
                                      nThread = 8,showProgress = TRUE) %>%
  mutate(length = length - 2)

dbWriteTable(pg,"uniref100_index_from_mmseqs_db", uniref100_index, overwrite = TRUE,field.types = c(id = "integer",offset = "bigint", length = "integer"))

rm(uniref100_index)
gc()
```

Read in MMSeqs database taxid mapping
```{r}
uniref100_taxid <- data.table::fread("data/reference/mmseqs2/uniref100_mapping", col.names = c("id", "taxid"),
                                      nThread = 8,showProgress = TRUE)

dbWriteTable(pg,"uniref100_taxid_from_mmseqs_db", uniref100_taxid, overwrite = TRUE,field.types = c(id = "integer",taxid = "integer"))

rm(uniref100_taxid)
gc()


```




## Read counts
```{r}
# Get read counts already loaded into the database
existing_read_counts <- tbl(pg, "read_count") %>% collect()

existing_read_count_samples <- get_pg_table_samples(pg, "read_count", "sample")

# Find read count tsvs
read_count_paths <- Sys.glob("data/omics/*/*/reads/*_read_count_fastp.tsv") %>% 
  data.frame(path = .) %>% 
  bind_cols(.,unglue::unglue_data(.$path,"data/omics/{sample_type}/{sample}/reads/{sample2}_read_count_fastp.tsv")) %>% 
  bind_cols(file.info(.$path))

# Filter read count tsvs to only those not already loaded in the database
new_read_count_paths <- read_count_paths %>% 
  filter(!sample %in% existing_read_count_samples$sample,
         sample == sample2)
```

```{r}
# Function to read count tsv and load into the database
read_tsv_to_pg <- function(path, sample) {
  states <- c("raw_reads", "deduped_reads", "filt_and_trimmed_reads", "decon_reads")

  print(sample)

  # Only used when creating the table for the first time, for all subsequent additions, append = TRUE should be added and the field types are already established
  postgres_col_types = c(read_state = 'text',
                         direction = 'text',
                         count = 'bigint',
                         sample = 'text',
                         percent_retained = 'numeric',
                         percent_removed = 'numeric',
                         method = 'text')
  
  
  tryCatch({
    read_tsv(path, col_types = "cnn") %>%
      pivot_longer(-read_state, names_to = "direction", values_to = "count") %>%
      mutate(sample = sample,
             direction = str_remove(direction, "_read_count"),
             read_state = factor(read_state, levels = states, ordered = TRUE)) %>%
      arrange(sample, direction, read_state) %>%
      group_by(sample, direction) %>%
      mutate(
        percent_retained = count / lag(count) * 100,
        percent_removed = (lag(count) - count) / lag(count) * 100,
        method = "Fastp"
      ) %>%
      dbWriteTable(pg, "read_count", ., overwrite = FALSE,
                   append = TRUE # Has to be false if table doesn't exit yet
                   #,field.types = postgres_col_types
                   )
  }, error = function(e) {
    cat("Error occurred while processing file:", path, "\n")
    cat("Error message:", conditionMessage(e), "\n")
  })
}

walk2(new_read_count_paths$path, new_read_count_paths$sample, read_tsv_to_pg)
```


# UniRef mapping to genes with Diamond
```{r}
diamond_uniref_contig_mapping <- tbl(pg, "diamond_uniref_contig_mapping")

diamond_mapped_samples <- get_pg_table_samples(pg, "diamond_uniref_contig_mapping", "sample")

uniref_contig_mappings <- Sys.glob("data/omics/metagenomes/*/*_GENES.m8") %>% 
  data.frame(path = .) %>% 
  mutate(sample = basename(path) %>% str_remove("_GENES.m8"))

new_uniref_contig_mappings <- uniref_contig_mappings %>% 
  filter(!sample %in% diamond_mapped_samples$sample)

read_uniref_contig_mapping <- function(path, sample_name){
  cols <- c("qseqid", "qlen", "sseqid", "slen", "qstart", "qend", "sstart", "send", "evalue", "pident", "mismatch", "qcovhsp", "scovhsp")
  
  mapping_res <- read_tsv(path,col_names = cols,show_col_types = FALSE) %>% 
    mutate(uniref100_id = str_remove(sseqid, "UniRef100_"),
           sample = sample_name)
  
  # Only used when creating the table for the first time, for all subsequent additions, append = TRUE should be added and the field types are already established
  postgres_col_types = c(qlen = 'integer',
                            slen = 'integer',
                            qstart = 'integer',
                            qend = 'integer',
                            sstart = 'integer',
                            send = 'integer',
                            pident = 'real',
                            mismatch = 'integer',
                            qcovhsp = 'real',
                            scovhsp = 'real',
                            evalue = 'numeric')
  
  # First run to create table
  # dbWriteTable(pg,"diamond_uniref_contig_mapping",mapping_res,overwrite = FALSE, field.types = postgres_col_types)
  
  # For appending new samples
  dbWriteTable(pg,"diamond_uniref_contig_mapping",mapping_res,overwrite = FALSE, append = TRUE)
  
  return(NULL)
}

walk2(new_uniref_contig_mappings$path, new_uniref_contig_mappings$sample, read_uniref_contig_mapping,.progress = TRUE)
```




```{r}
sample <- "samp_447"
sample_type <- "metagenomes"

header_names <- c("qseqid", "qlen", "sseqid", "slen", "qstart", "qend", "sstart", "send", "evalue", "pident", "mismatch", "qcovhsp", "scovhsp")

diamond_hits <- read_tsv(str_glue("data/omics/{sample_type}/{sample}/{sample}_GENES.m8"),col_names = header_names)

filt_hits <- diamond_hits %>% 
  group_by(qseqid) %>% 
  slice_min(evalue)

filt_hits_w_contig <- filt_hits %>% 
  mutate(contig = str_remove(qseqid, "_[:digit:]$"))
```



#### Read mapping to UniRef results from mmseqs2

```{r}
mmseqs_uniref_mapping <- tbl(pg, "read_mapping_to_uniref")

mapped_samples <- get_pg_table_samples(pg, "read_mapping_to_uniref", "sample")

# mapped_samples <- mmseqs_uniref_mapping %>% 
#   select("sample") %>% 
#   distinct() %>% 
#   collect() %>% 
#   pull("sample")

uniref_read_mappings <- Sys.glob("data/omics/metagenomes/*/*_tophit_report") %>% 
  data.frame(path = .) %>% 
  unglue::unglue_unnest(path, "data/omics/metagenomes/{sample}/{sample2}_tophit_report",remove = FALSE)
  #mutate(sample = basename(path) %>% str_remove("_tophit_report"))

new_uniref_read_mappings <- uniref_read_mappings %>% 
  filter(!str_detect(path, "contig_tophit_report"),
         !sample %in% mapped_samples$sample,
         str_detect(sample, "^samp_*")) %>% 
  mutate(fs::file_info(path))

read_uniref_read_tophit_report <- function(path, sample_name){
  cols <- c("target", "num_seqs_aligned", "unique_coverage_of_target", "target_coverage", "average_seq_identity", "taxonomy", "rank","tax_name", "lineage")
  
  report <- read_tsv(path,col_names = cols,show_col_types = FALSE) %>% 
    mutate(uniref100_id = str_remove(target, "UniRef100_"),
           sample = sample_name) %>% 
    select(-c("rank","tax_name", "lineage"))
  
  
  # # Only used when creating the table for the first time, for all subsequent additions, 
  # # append = TRUE should be added as the field types are already established
  # postgres_col_types = c(target = 'text',
  #                           num_seqs_aligned = 'integer',
  #                           unique_coverage_of_target = 'numeric',
  #                           target_coverage = 'numeric',
  #                           average_seq_identity = 'numeric',
  #                           taxonomy = 'integer',
  #                           uniref100_id = 'real',
  #                           sample = 'integer')
  
  # First run to create table
  # dbWriteTable(pg,"diamond_uniref_contig_mapping",mapping_res,overwrite = FALSE, field.types = postgres_col_types)
  
  
  dbWriteTable(pg,"read_mapping_to_uniref",report,overwrite = FALSE, append = TRUE)
  
  return(NULL)
}

message("Dropping indices") # Much faster to drop indices and rebuild than to insert new rows leaving existing indices on

# # For the indices created in DBeaver
# sql('DROP INDEX public.read_mapping_to_uniref_sample_idx') %>%  DBI::dbSendQuery(pg, .)
# sql('DROP INDEX public.read_mapping_to_uniref_uniref100_id_idx') %>% DBI::dbSendQuery(pg, .)
# sql('DROP INDEX public.read_mapping_to_uniref_taxonomy_idx') %>% DBI::dbSendQuery(pg, .)

# For the indexes created by this script
sql('DROP INDEX public.read_mapping_to_uniref_sample') %>% DBI::dbSendQuery(pg, .)
sql('DROP INDEX public.read_mapping_to_uniref_uniref100_id') %>% DBI::dbSendQuery(pg, .)
sql('DROP INDEX public.read_mapping_to_uniref_taxonomy') %>% DBI::dbSendQuery(pg, .)

message("Loading new mapping results")
walk2(new_uniref_read_mappings$path, new_uniref_read_mappings$sample, read_uniref_read_tophit_report,.progress = TRUE)

message("Re-creating indices")
sql('CREATE INDEX read_mapping_to_uniref_sample ON public.read_mapping_to_uniref (sample)') %>%  DBI::dbSendQuery(pg, .)
sql('CREATE INDEX read_mapping_to_uniref_uniref100_id ON public.read_mapping_to_uniref (uniref100_id)') %>%  DBI::dbSendQuery(pg, .)
sql('CREATE INDEX read_mapping_to_uniref_taxonomy ON public.read_mapping_to_uniref (taxonomy)') %>%  DBI::dbSendQuery(pg, .)
```


#### Contig UniRef tophit summary from mmseqs2

```{r}
mmseqs_uniref_mapping <- tbl(pg, "contig_uniref_tophits")

mapped_samples <- get_pg_table_samples(pg, "contig_uniref_tophits", "sample")

# mapped_samples <- mmseqs_uniref_mapping %>% 
#   select("sample") %>% 
#   distinct() %>% 
#   collect() %>% 
#   pull("sample")

uniref_read_mappings <- Sys.glob("data/omics/metagenomes/*/*_contig_tophit_report") %>% 
  data.frame(path = .) %>% 
  unglue::unglue_unnest(path, "data/omics/metagenomes/{sample}/{sample2}_contig_tophit_report",remove = FALSE)

new_uniref_read_mappings <- uniref_read_mappings %>% 
  filter(!sample %in% mapped_samples$sample,
         str_detect(sample, "^samp_*")) %>%
  mutate(fs::file_info(path))

read_uniref_read_tophit_report <- function(path, sample_name){
  cols <- c("target", "num_seqs_aligned", "unique_coverage_of_target", "target_coverage", "average_seq_identity", "taxonomy", "rank","tax_name", "lineage")
  
  report <- read_tsv(path,col_names = cols,show_col_types = FALSE) %>% 
    mutate(uniref100_id = str_remove(target, "UniRef100_"),
           sample = sample_name) %>% 
    select(-c("rank","tax_name", "lineage"))
  
  
  # # Only used when creating the table for the first time, for all subsequent additions, 
  # # append = TRUE should be added as the field types are already established
  # postgres_col_types = c(target = 'text',
  #                           num_seqs_aligned = 'integer',
  #                           unique_coverage_of_target = 'numeric',
  #                           target_coverage = 'numeric',
  #                           average_seq_identity = 'numeric',
  #                           taxonomy = 'integer',
  #                           uniref100_id = 'text',
  #                           sample = 'text')

  # First run to create table
   # dbWriteTable(pg,"contig_uniref_tophits",report,overwrite = FALSE, field.types = postgres_col_types)
  
  dbWriteTable(pg,"contig_uniref_tophits",report,overwrite = FALSE, append = TRUE)
  
  return(NULL)
}

message("Dropping indices") # Much faster to drop indices and rebuild than to insert new rows leaving existing indices on

pg <- DBI::dbConnect(RPostgres::Postgres(),dbname = "glamr_data", host = "cayman.earth.lsa.umich.edu", port = "5432", user = "glamr_admin", password = "glamr2023")

# For the indexes created by this script
sql('DROP INDEX public.contig_uniref_tophits_sample') %>% DBI::dbSendQuery(pg, .)
sql('DROP INDEX public.contig_uniref_tophits_uniref100_id') %>% DBI::dbSendQuery(pg, .)
sql('DROP INDEX public.contig_uniref_tophits_taxonomy') %>% DBI::dbSendQuery(pg, .)

message("Loading new mapping results")
walk2(new_uniref_read_mappings$path, new_uniref_read_mappings$sample, read_uniref_read_tophit_report,.progress = TRUE)

message("Re-creating indices")
sql('CREATE INDEX contig_uniref_tophits_sample ON public.contig_uniref_tophits (sample)') %>%  DBI::dbSendQuery(pg, .)
sql('CREATE INDEX contig_uniref_tophits_uniref100_id ON public.contig_uniref_tophits (uniref100_id)') %>%  DBI::dbSendQuery(pg, .)
sql('CREATE INDEX contig_uniref_tophits_taxonomy ON public.contig_uniref_tophits (taxonomy)') %>%  DBI::dbSendQuery(pg, .)
```

#### Contig UniRef tophits alignments from mmseqs2

```{r}
mmseqs_uniref_mapping <- tbl(pg, "contig_uniref_tophit_alignments")

mapped_samples <- get_pg_table_samples(pg, "contig_uniref_tophit_alignments", "sample")

# mapped_samples <- mmseqs_uniref_mapping %>% 
#   select("sample") %>% 
#   distinct() %>% 
#   collect() %>% 
#   pull("sample")

uniref_read_mappings <- Sys.glob("data/omics/metagenomes/*/*_contig_tophit_aln") %>% 
  data.frame(path = .) %>% 
  unglue::unglue_unnest(path, "data/omics/metagenomes/{sample}/{sample2}_contig_tophit_aln",remove = FALSE)

new_uniref_read_mappings <- uniref_read_mappings %>% 
  filter(!sample %in% mapped_samples$sample,
         str_detect(sample, "^samp_*")) %>%
  mutate(fs::file_info(path))

read_uniref_read_tophit_alignments <- function(path, sample_name){
  cols <- c("contig","target", "percent_identity", "alignment_length", "num_mismatches", "num_gaps", "contig_start", "contig_end","target_start","target_end","e_value", "bit_score")
  
  report <- read_tsv(path,col_names = cols,show_col_types = FALSE) %>% 
    mutate(uniref100_id = str_remove(target, "UniRef100_"),
           sample = sample_name)
  
  
  # # Only used when creating the table for the first time, for all subsequent additions, 
  # # append = TRUE should be added as the field types are already established
  # postgres_col_types = c(contig = "text",
  #                        target = "text", 
  #                        percent_identity = "numeric", 
  #                        alignment_length ="int", 
  #                        num_mismatches = "int", 
  #                        num_gaps = "int", 
  #                        contig_start = "int", 
  #                        contig_end = "int",
  #                        target_start = "int",
  #                        target_end = "int",
  #                        e_value = "numeric", 
  #                        bit_score = "numeric")
  # 
  # # First run to create table
  #  dbWriteTable(pg,"contig_uniref_tophit_alignments",report,overwrite = FALSE, field.types = postgres_col_types)
  
  dbWriteTable(pg,"contig_uniref_tophit_alignments",report,overwrite = FALSE, append = TRUE)
  
  return(NULL)
}

message("Dropping indices") # Much faster to drop indices and rebuild than to insert new rows leaving existing indices on

pg <- DBI::dbConnect(RPostgres::Postgres(),dbname = "glamr_data", host = "cayman.earth.lsa.umich.edu", port = "5432", user = "glamr_admin", password = "glamr2023")

# For the indexes created by this script
sql('DROP INDEX public.contig_uniref_tophit_alignments_sample') %>% DBI::dbSendQuery(pg, .)
sql('DROP INDEX public.contig_uniref_tophit_alignments_uniref100_id') %>% DBI::dbSendQuery(pg, .)
sql('DROP INDEX public.contig_uniref_tophit_alignments_contig') %>% DBI::dbSendQuery(pg, .)

message("Loading new contig tophit alignments")
walk2(new_uniref_read_mappings$path, new_uniref_read_mappings$sample, read_uniref_read_tophit_alignments,.progress = TRUE)

message("Re-creating indices")
sql('CREATE INDEX contig_uniref_tophit_alignments_sample ON public.contig_uniref_tophit_alignments (sample)') %>%  DBI::dbSendQuery(pg, .)
sql('CREATE INDEX contig_uniref_tophit_alignments_uniref100_id ON public.contig_uniref_tophit_alignments (uniref100_id)') %>%  DBI::dbSendQuery(pg, .)
sql('CREATE INDEX contig_uniref_tophit_alignments_contig ON public.contig_uniref_tophit_alignments (contig)') %>%  DBI::dbSendQuery(pg, .)
```


#### Contig LCAs from mmseqs LCA on UniRef

```{r}
contig_lca_uniref <- tbl(pg, "contig_lca_uniref")

lca_samples <- get_pg_table_samples(pg, "contig_lca_uniref", "sample")

# mapped_samples <- mmseqs_uniref_mapping %>% 
#   select("sample") %>% 
#   distinct() %>% 
#   collect() %>% 
#   pull("sample")

contig_lca_annotation_files <- Sys.glob("data/omics/*/*/*_contig_lca.tsv") %>% 
  data.frame(path = .) %>% 
  unglue::unglue_unnest(path, "data/omics/{sample_type}/{sample}/{sample2}_contig_lca.tsv",remove = FALSE)

new_contig_lca_annotation_files <- contig_lca_annotation_files %>% 
  filter(!sample %in% lca_samples$sample,
         str_detect(sample, "^samp_*") | str_detect(sample, "^coassembly_") | str_detect(sample, "^GVHDsamp_")) %>% 
  mutate(fs::file_info(path))

read_contig_lca <- function(path, sample_name){
  cols <- c("contig", "taxonomy", "rank", "tax_name", "num_frags_retained", "num_frags_tax_assigned", "num_frags_agree","support", "lineage")
  
  report <- read_tsv(path,col_names = cols,show_col_types = FALSE) %>% 
    mutate(sample = sample_name) %>% 
    dplyr::select(-c("rank","tax_name", "lineage"))
  
  
  # # Only used when creating the table for the first time, for all subsequent additions, 
  # # append = TRUE should be added as the field types are already established
  # postgres_col_types = c(contig = 'text',
  #                           taxonomy = 'integer',
  #                           num_frags_retained = 'integer',
  #                           num_frags_tax_assigned = 'integer',
  #                           num_frags_agree = 'integer',
  #                           support = 'numeric')
  # 
  # First run to create table
  #dbWriteTable(pg,"contig_lca_uniref",report,overwrite = FALSE, field.types = postgres_col_types)
  
  
  dbWriteTable(pg,"contig_lca_uniref",report,overwrite = FALSE, append = TRUE)
  
  return(NULL)
}

# For the indexes created by this script
sql('DROP INDEX public.contig_lca_uniref_sample') %>% DBI::dbSendQuery(pg, .)
sql('DROP INDEX public.contig_lca_uniref_contig') %>% DBI::dbSendQuery(pg, .)
sql('DROP INDEX public.contig_lca_uniref_taxonomy') %>% DBI::dbSendQuery(pg, .)

walk2(new_contig_lca_annotation_files$path, new_contig_lca_annotation_files$sample, read_contig_lca,.progress = TRUE)

sql('CREATE INDEX "contig_lca_uniref_sample" ON "contig_lca_uniref" ("sample")') %>% DBI::dbSendQuery(pg, .)
sql('CREATE INDEX "contig_lca_uniref_contig" ON "contig_lca_uniref" ("contig")') %>%  DBI::dbSendQuery(pg, .)
sql('CREATE INDEX "contig_lca_uniref_taxonomy" ON "contig_lca_uniref" ("taxonomy")') %>% DBI::dbSendQuery(pg, .)
```


## Community summaries from read mapping to UniRef

```{r}
read_mapping_LCA_summary <- tbl(pg, "read_mapping_LCA_summary")

mapped_samples <- get_pg_table_samples(pg, "read_mapping_LCA_summary", "SampleID")

# mapped_samples <- mmseqs_uniref_mapping %>% 
#   select("sample") %>% 
#   distinct() %>% 
#   collect() %>% 
#   pull("sample")

uniref_LCA_reports <- Sys.glob("data/omics/metagenomes/*/*_report_w_standardized_lineage") %>% 
  data.frame(path = .) %>% 
  unglue::unglue_unnest(path, "data/omics/metagenomes/{sample}/{sample2}_report_w_standardized_lineage",remove = FALSE)

new_uniref_LCA_reports <- uniref_LCA_reports %>% 
  filter(!sample %in% mapped_samples$sample,
         str_detect(sample, "^samp_*")) %>% 
  mutate(fs::file_info(path))

#tax_of_interest <- googlesheets4::read_sheet("https://docs.google.com/spreadsheets/d/1izMOKyA6ecy2lZsXLoXGnftcyFDdlQI39ncUXnfDw_Y/edit#gid=0")

read_report <- function(path, samp_name){
  cols <- c("percent_and_below", "count_and_below", "count_directly", "rank", "tax_id", "tax_name", "lineage_full", "lineage_std")
  levels <- c("Kingdom", "Phylum", "Class", "Order", "Family", "Genus", "Species")
  
  report <- data.table::fread(path, col.names = cols) %>% 
    select(-count_directly, -count_and_below) %>% 
    dplyr::rename(percent_abundance = "percent_and_below") %>% 
    mutate(SampleID = samp_name) %>% 
    separate(lineage_std, into = levels, sep = ";[A-z]__", remove = FALSE) %>% 
    mutate(Kingdom = str_remove(Kingdom,"k__"))
  
  dbWriteTable(pg,"read_mapping_LCA_summary",report,overwrite = FALSE, append = TRUE)
  
  return(NULL)
}


# For the indexes created by this script
#sql('DROP INDEX public.read_mapping_LCA_summary') %>% DBI::dbSendQuery(pg, .)
#sql('DROP INDEX public.read_mapping_LCA_summary') %>% DBI::dbSendQuery(pg, .)

community_comp <- walk2(uniref_LCA_reports$path, uniref_LCA_reports$sample, read_report)

sql('CREATE INDEX "read_mapping_LCA_summary_sample" ON "read_mapping_LCA_summary" ("SampleID")') %>% DBI::dbSendQuery(pg, .)
sql('CREATE INDEX "read_mapping_LCA_summary_taxonomy" ON "read_mapping_LCA_summary" ("tax_id")') %>% DBI::dbSendQuery(pg, .)
sql('CREATE INDEX "read_mapping_LCA_summary_genus" ON "read_mapping_LCA_summary" ("Genus")') %>% DBI::dbSendQuery(pg, .)
```


## Kraken: GTDB
```{r}
# Get GTDB annotations that are already loaded into the database
existing_krakenGTDB_reports <- get_pg_table_samples(pg, "krakenGTDB", "sample")

# Find all local results
krakenGTDB_reports_local <- Sys.glob("data/omics/*/*/kraken_fastp/gtdb_*_brackenReport.txt") %>% 
  data.frame(path = .) %>% 
  bind_cols(.,unglue::unglue_data(.$path,"data/omics/{sample_type}/{sample}/kraken_fastp/gtdb_{sample2}_brackenReport.txt")) %>% 
  bind_cols(file.info(.$path))

# Filter list of summary tsvs to only those not already loaded into the database
new_krakenGTDB_reports <- krakenGTDB_reports_local %>% 
  filter(
      !sample %in% existing_krakenGTDB_reports$sample & (str_detect(sample, "^samp_") | str_detect(sample, "^coassembly_") | str_detect(sample, "^GVHDsamp_")))

# Load an example summary tsv to get the column types
abund_test <- read_tsv("data/omics/metagenomes/samp_447/kraken_fastp/gtdb_E20150029_brackenReport.txt",
                       col_names = c("percent_abund","count_w_subtax", "count_direct","rank","tax_id","name")) %>% 
  mutate(sample = "samp_447",name = str_remove(name, "^[a-z]__"))
column_types <- as.col_spec(abund_test)

# First upload to establish the table
    # postgres_col_types = c(percent_abund = "numeric",
    #                        count_w_subtax = "bigint",
    #                        count_direct = "bigint",
    #                        rank = "char",
    #                        tax_id = "int",
    #                        name = "text"
    #                        )
    # 
    # read_tsv("data/omics/metagenomes/samp_447/kraken_fastp/gtdb_E20150029_brackenReport.txt",
    #                    col_names = c("percent_abund","count_w_subtax", "count_direct","rank","tax_id","name")) %>% 
    #   mutate(sample = "samp_447",name = str_remove(name, "^[a-z]__")) %>% 
    #   relocate(sample) %>%
    #   dbWriteTable(pg, "krakenGTDB", ., overwrite = FALSE, field.types = postgres_col_types)

# Function for reading the summary tsv and saving it to the database
read_krakenGTDB_to_pg <- function(path, sample) {
  print(sample)
  tryCatch({
    read_tsv(path,
             col_names = c("percent_abund","count_w_subtax", "count_direct","rank","tax_id","name")) %>% 
      mutate(sample = sample,name = str_remove(name, "^[a-z]__")) %>% 
      relocate(sample) %>% 
      dbWriteTable(pg, "krakenGTDB", ., overwrite = FALSE, append = TRUE)
  }, error = function(e) {
    cat("Error occurred while processing file:", path, "\n")
    cat("Error message:", conditionMessage(e), "\n")
  })
}

#test <- read_drep_to_pg(drep_paths$path[1],sample = drep_paths$sample[1])


# Load new summaries into the database
# Drop existing indices [speeds up loading of new data]
# sql('DROP INDEX public.krakenGTDB_tax_id') %>% DBI::dbSendQuery(pg, .)
# sql('DROP INDEX public.krakenGTDBsample') %>% DBI::dbSendQuery(pg, .)
# sql('DROP INDEX public.krakenGTDBname') %>% DBI::dbSendQuery(pg, .)

# Append new data
walk2(new_krakenGTDB_reports$path, 
      new_krakenGTDB_reports$sample, 
      read_krakenGTDB_to_pg,
      .progress = TRUE)

# Recreate indices
sql('CREATE INDEX "krakenGTDB_tax_id" ON "krakenGTDB" ("tax_id")') %>% DBI::dbSendQuery(pg, .)
sql('CREATE INDEX "krakenGTDBsample" ON "krakenGTDB" ("sample")') %>% DBI::dbSendQuery(pg, .)
sql('CREATE INDEX "krakenGTDBname" ON "krakenGTDB" ("name")') %>% DBI::dbSendQuery(pg, .)
```
## Kraken: REFSEQ
```{r}
# Get GTDB annotations that are already loaded into the database
existing_krakenREFSEQ_reports <- get_pg_table_samples(pg, "krakenREFSEQ", "sample")

# Find all local results
krakenREFSEQ_reports_local <- Sys.glob("data/omics/*/*/kraken_fastp/refseq_*_brackenReport.txt") %>% 
  data.frame(path = .) %>% 
  bind_cols(.,unglue::unglue_data(.$path,"data/omics/{sample_type}/{sample}/kraken_fastp/refseq_{sample2}_brackenReport.txt")) %>% 
  bind_cols(file.info(.$path))

# Filter list of summary tsvs to only those not already loaded into the database
new_krakenREFSEQ_reports <- krakenREFSEQ_reports_local %>% 
  filter(
      !sample %in% existing_krakenREFSEQ_reports$sample & (str_detect(sample, "^samp_") | str_detect(sample, "^coassembly_") | str_detect(sample, "^GVHDsamp_")))

# Load an example summary tsv to get the column types
abund_test <- read_tsv("data/omics/metagenomes/samp_447/kraken_fastp/refseq_E20150029_brackenReport.txt",
                       col_names = c("percent_abund","count_w_subtax", "count_direct","rank","tax_id","name")) %>% 
  mutate(sample = "samp_447",name = str_remove(name, "^[a-z]__"))
column_types <- as.col_spec(abund_test)

# First upload to establish the table
    # postgres_col_types = c(percent_abund = "numeric",
    #                        count_w_subtax = "bigint",
    #                        count_direct = "bigint",
    #                        rank = "text",
    #                        tax_id = "int",
    #                        name = "text"
    #                        )
    # 
    # read_tsv("data/omics/metagenomes/samp_447/kraken_fastp/refseq_E20150029_brackenReport.txt",
    #                    col_names = c("percent_abund","count_w_subtax", "count_direct","rank","tax_id","name")) %>%
    #   mutate(sample = "samp_447",name = str_remove(name, "^[a-z]__")) %>%
    #   relocate(sample) %>%
    #   dbWriteTable(pg, "krakenREFSEQ", ., overwrite = FALSE, field.types = postgres_col_types)

# Function for reading the summary tsv and saving it to the database
read_krakenREFSEQ_to_pg <- function(path, sample) {
  print(sample)
  tryCatch({
    read_tsv(path,
             col_names = c("percent_abund","count_w_subtax", "count_direct","rank","tax_id","name")) %>% 
      mutate(sample = sample,name = str_remove(name, "^[a-z]__")) %>% 
      relocate(sample) %>% 
      dbWriteTable(pg, "krakenREFSEQ", ., overwrite = FALSE, append = TRUE)
  }, error = function(e) {
    cat("Error occurred while processing file:", path, "\n")
    cat("Error message:", conditionMessage(e), "\n")
  })
}

#test <- read_drep_to_pg(drep_paths$path[1],sample = drep_paths$sample[1])


# Load new summaries into the database
# Drop existing indices [speeds up loading of new data]
sql('DROP INDEX public.krakenREFSEQ_tax_id') %>% DBI::dbSendQuery(pg, .)
sql('DROP INDEX public.krakenREFSEQsample') %>% DBI::dbSendQuery(pg, .)
sql('DROP INDEX public.krakenREFSEQname') %>% DBI::dbSendQuery(pg, .)

# Append new data
walk2(new_krakenREFSEQ_reports$path, 
      new_krakenREFSEQ_reports$sample, 
      read_krakenREFSEQ_to_pg,
      .progress = TRUE)

# Recreate indices
sql('CREATE INDEX "krakenREFSEQ_tax_id" ON "krakenREFSEQ" ("tax_id")') %>% DBI::dbSendQuery(pg, .)
sql('CREATE INDEX "krakenREFSEQsample" ON "krakenREFSEQ" ("sample")') %>% DBI::dbSendQuery(pg, .)
sql('CREATE INDEX "krakenREFSEQname" ON "krakenREFSEQ" ("name")') %>% DBI::dbSendQuery(pg, .)
```

## GTDB
```{r}
# Get GTDB annotations that are already loaded into the database
existing_gtdb_annotations <- tbl(pg, "GTDB") %>% 
  select(sample) %>% 
  distinct() %>% 
  collect() 

# Find all GTDB summary tsvs
GTDB_paths <- Sys.glob("data/omics/metagenomes/*/bins/GTDB/*.summary.tsv") %>% 
  data.frame(path = .) %>% 
  bind_cols(.,unglue::unglue_data(.$path,"data/omics/{sample_type}/{sample}/bins/GTDB/{bac_or_arc}.summary.tsv")) %>% 
  bind_cols(file.info(.$path))

# Filter list of summary tsvs to only those not already loaded into the database
new_GTDB <- GTDB_paths %>% 
  filter(!sample %in% existing_gtdb_annotations$sample,
         str_detect(sample, "^samp_") | str_detect(sample, "^coassembly_") | str_detect(sample, "^GVHDsamp_"))

# Load an example summary tsv to get the column types
gtdb_test <- read_tsv("data/omics/metagenomes/samp_2015/bins/GTDB/gtdbtk.bac120.summary.tsv",na = "N/A")
column_types <- as.col_spec(gtdb_test)

# Function for reading the summary tsv and saving it to the database
read_gtdb_to_pg <- function(path, sample) {

  print(sample)

  tryCatch({
    read_tsv(path, na = "N/A",col_types = column_types) %>%
      separate(classification, into = c("domain","phylum", "class", "order", "family","genus","species"),sep = ";[a-z]__",remove = FALSE) %>% 
      mutate(domain = str_remove(domain,"d__"),
             sample = sample) %>% 
      dplyr::rename(bin ="user_genome") %>% 
      relocate(sample) %>% 
      dbWriteTable(pg, "GTDB", ., overwrite = FALSE, append = TRUE)
  }, error = function(e) {
    cat("Error occurred while processing file:", path, "\n")
    cat("Error message:", conditionMessage(e), "\n")
  })
}

sql('DROP INDEX public.gtdb_bin') %>% DBI::dbSendQuery(pg, .)
sql('DROP INDEX public.gtdb_sample') %>% DBI::dbSendQuery(pg, .)

# Load new summaries into the database
walk2(new_GTDB$path, new_GTDB$sample, read_gtdb_to_pg)

sql('CREATE INDEX "gtdb_bin" ON "GTDB" ("bin")') %>% DBI::dbSendQuery(pg, .)
sql('CREATE INDEX "gtdb_sample" ON "GTDB" ("sample")') %>% DBI::dbSendQuery(pg, .)

```

## CheckM
```{r}
existing_checkM_summaries <- tbl(pg, "checkm") %>% 
  select(sample) %>% 
  distinct() %>% 
  collect() 

checkm_paths <- Sys.glob("data/omics/metagenomes/*/bins/all_raw_bins/checkm.txt") %>% 
  data.frame(path = .) %>% 
  bind_cols(.,unglue::unglue_data(.$path,"data/omics/metagenomes/{sample}/bins/all_raw_bins/checkm.txt")) %>% 
  bind_cols(file.info(.$path))

new_checkM <- checkm_paths %>% 
  filter(!sample %in% existing_checkM_summaries$sample,
         str_detect(sample, "^samp_") | str_detect(sample, "^coassembly_") | str_detect(sample, "^GVHDsamp_"))

checkm_test <- read_tsv("data/omics/metagenomes/samp_2037/bins/all_raw_bins/checkm.txt")
checkm_column_types <- as.col_spec(checkm_test)

read_checkm_to_pg <- function(path, sample) {

  print(sample)

  tryCatch({
    read_tsv(path,col_types = checkm_column_types) %>%
      dplyr::rename(bin = "Bin Id") %>% 
      dplyr::mutate(sample = sample) %>%
      dplyr::relocate(sample) %>% 
      dbWriteTable(pg, "checkm", ., overwrite = FALSE, append = TRUE)
  }, error = function(e) {
    cat("Error occurred while processing file:", path, "\n")
    cat("Error message:", conditionMessage(e), "\n")
  })
}

sql('DROP INDEX public.checkm_bin') %>% DBI::dbSendQuery(pg, .)
sql('DROP INDEX public.checkm_sample') %>% DBI::dbSendQuery(pg, .)

walk2(new_checkM$path, new_checkM$sample, read_checkm_to_pg)

sql('CREATE INDEX "checkm_bin" ON "checkm" ("bin")') %>% DBI::dbSendQuery(pg, .)
sql('CREATE INDEX "checkm_sample" ON "checkm" ("sample")') %>% DBI::dbSendQuery(pg, .)

```


## DREP
```{r}
# Get DREP results that are already loaded into the database
existing_drep_annotations <- tbl(pg, "drep") %>% 
  select(sample) %>% 
  distinct() %>% 
  collect() 

# Find all DREP results
drep_paths <- Sys.glob("data/omics/metagenomes/*/bins/drep/data_tables/Cdb.csv") %>% 
  data.frame(path = .) %>% 
  bind_cols(.,unglue::unglue_data(.$path,"data/omics/{sample_type}/{sample}/bins/drep/data_tables/Cdb.csv")) %>% 
  bind_cols(file.info(.$path))

# Filter list of summary tsvs to only those not already loaded into the database
new_drep <- drep_paths %>% 
  filter(
      !sample %in% existing_drep_annotations$sample,
      (str_detect(sample, "^samp_") | str_detect(sample, "^coassembly_") | str_detect(sample, "^GVHDsamp_")))

# Load an example summary tsv to get the column types
drep_test <- read_csv("data/omics/metagenomes/samp_447/bins/drep/data_tables/Cdb.csv") %>% 
  mutate(is_cluster_rep = fs::file_exists(str_glue("data/omics/metagenomes/samp_447/bins/drep/dereplicated_genomes/{genome}")))
column_types <- as.col_spec(drep_test)

# Function for reading the summary tsv and saving it to the database
read_drep_to_pg <- function(path, sample) {
  print(sample)
  tryCatch({
    read_csv(path,show_col_types = FALSE) %>%
      mutate(drep_folder = str_remove(path, '/data_tables/Cdb.csv'),
             is_cluster_rep = fs::file_exists(str_glue("{drep_folder}/dereplicated_genomes/{genome}")),
             genome = str_remove(genome,".fa"),
             sample = sample) %>% 
      dplyr::rename(bin = "genome") %>% 
      select(-drep_folder) %>% 
      dplyr::relocate(sample) %>% 
      dbWriteTable(pg, "drep", ., overwrite = FALSE, append = TRUE)
  }, error = function(e) {
    cat("Error occurred while processing file:", path, "\n")
    cat("Error message:", conditionMessage(e), "\n")
  })
}

#test <- read_drep_to_pg(drep_paths$path[1],sample = drep_paths$sample[1])


# Load new summaries into the database
# Drop existing indices [speeds up loading of new data]
sql('DROP INDEX public.drep_bin') %>% DBI::dbSendQuery(pg, .)
sql('DROP INDEX public.drep_rep') %>% DBI::dbSendQuery(pg, .)
sql('DROP INDEX public.drep_cluster') %>% DBI::dbSendQuery(pg, .)
sql('DROP INDEX public.drep_sample') %>% DBI::dbSendQuery(pg, .)

# Append new data
walk2(new_drep$path, new_drep$sample, read_drep_to_pg)

# Recreate indices
sql('CREATE INDEX "drep_bin" ON "drep" ("bin")') %>% DBI::dbSendQuery(pg, .)
sql('CREATE INDEX "drep_rep" ON "drep" ("is_cluster_rep")') %>% DBI::dbSendQuery(pg, .)
sql('CREATE INDEX "drep_cluster" ON "drep" ("secondary_cluster")') %>% DBI::dbSendQuery(pg, .)
sql('CREATE INDEX "drep_sample" ON "drep" ("sample")') %>% DBI::dbSendQuery(pg, .)
```


# Contig bin membership
```{r}
contig_bin_membership <- tbl(pg, "contig_bin_membership")

bin_membership_samples <- get_pg_table_samples(pg, "contig_bin_membership", "sample")

# mapped_samples <- contig_bin_membership %>% 
#   select("sample") %>% 
#   distinct() %>% 
#   collect() %>% 
#   pull("sample")

bin_membership_files <- Sys.glob("data/omics/metagenomes/*/bins/contig_bins.rds") %>% 
  data.frame(path = .) %>% 
  unglue::unglue_unnest(path, "data/omics/metagenomes/{sample}/bins/contig_bins.rds",remove = FALSE) %>% 
  mutate(file_size = fs::file_size(path))

new_bin_membership_files <- bin_membership_files %>% 
  filter(!sample %in% bin_membership_samples$sample,
         str_detect(sample, "^samp_*") | str_detect(sample, "^coassembly_*") | str_detect(sample, "^GVHDsamp_")) %>% 
  mutate(fs::file_info(path))

read_contig_bin_membership <- function(path){
  
  contig_mem_df <- read_rds(path) %>% 
    dplyr::select(contig, sample, bin = "new_bin_name", length) %>% 
    distinct()
  
  # Only used when creating the table for the first time, for all subsequent additions,
  # append = TRUE should be added as the field types are already established
  # postgres_col_types = c(contig = 'text',
  #                           sample = 'text',
  #                           bin = 'text',
  #                           length = 'integer')
  # 
  # # First run to create table
  #  dbWriteTable(conn = pg,
  #               name = "contig_bin_membership",
  #               contig_mem_df,
  #               overwrite = FALSE, 
  #               field.types = postgres_col_types)

  dbWriteTable(conn = pg,
               name = "contig_bin_membership",
               value = contig_mem_df,
               overwrite = FALSE, append = TRUE)
  
  return(NULL)
}

# Drop indices to load faster
# For the indexes created by this script
sql('DROP INDEX contig_bin_membership_sample') %>% DBI::dbSendQuery(pg, .)
sql('DROP INDEX contig_bin_membership_bin') %>%  DBI::dbSendQuery(pg, .)
sql('DROP INDEX contig_bin_membership_contig') %>% DBI::dbSendQuery(pg, .)

walk(new_bin_membership_files$path, read_contig_bin_membership,.progress = TRUE)

# Create new indices
sql('CREATE INDEX "contig_bin_membership_sample" ON "contig_bin_membership" ("sample")') %>%  DBI::dbSendQuery(pg, .)
sql('CREATE INDEX "contig_bin_membership_bin" ON "contig_bin_membership" ("bin")') %>% DBI::dbSendQuery(pg, .)
sql('CREATE INDEX "contig_bin_membership_contig" ON "contig_bin_membership" ("contig")') %>% DBI::dbSendQuery(pg, .)

```

# Gene abundance results

```{r}
gene_abundance <- tbl(pg, "gene_abundance")

gene_abundance_samples <- get_pg_table_samples(pg, "gene_abundance", "sample")

gene_abundance_files <- Sys.glob("data/omics/metagenomes/*/genes/*_READSvsGENES.rpkm") %>% 
  data.frame(path = .) %>% 
  unglue::unglue_unnest(path, "data/omics/metagenomes/{sample}/genes/{sample2}_READSvsGENES.rpkm",remove = FALSE) %>% 
  mutate(file_size = fs::file_size(path))

new_gene_abundance_files <- gene_abundance_files %>% 
  filter(!sample %in% gene_abundance_samples$sample,
         str_detect(sample, "^samp_*") | str_detect(sample, "^coassembly_*") | str_detect(sample, "^GVHDsamp_")) %>% 
  mutate(fs::file_info(path))

#path <- "data/omics/metagenomes/samp_447/genes/samp_447_READSvsGENES.rpkm"
#sample_name = "samp_447"

read_gene_abundance <- function(path, sample_name){
  
  gene_abund_df <- read_tsv(path,skip = 4,show_col_types = FALSE) %>% 
    dplyr::rename(gene_header = "#Name") %>% 
    mutate(tpm = (FPKM / sum(FPKM))*10^6) %>% 
    separate(gene_header, into = c("gene", "left_edge","right_edge", "strand", "id"),sep = " # ") %>% 
    separate(id, into = c("id", "partial","start_type", "rbs_motif", "rbs_spacer","gc_content"), sep = ";") %>% 
    mutate(across(c("id", "partial","start_type", "rbs_motif", "rbs_spacer","gc_content"), ~str_remove(.x, "^.*="))) %>% 
    separate(id, into = c("contig_num", "gene_num"), sep = "_",remove = FALSE) %>% 
    mutate(sample = sample_name,
           contig = str_glue("{sample}_{contig_num}")) %>% 
    type_convert() %>% 
    dplyr::rename(length = "Length", bases = "Bases", coverage = "Coverage", reads = "Reads", rpkm = "RPKM", frags = "Frags", fpkm = "FPKM") %>% 
    relocate(gene, contig, sample) %>% 
    dplyr::select(-id)

  
  
  
  # Only used when creating the table for the first time, for all subsequent additions,
  # append = TRUE should be added as the field types are already established
  postgres_col_types = c(gene = 'text',
                         sample = "text",
                         contig = "text",
                         contig_num = "bigint",
                         gene_num = "int",
                         left_edge = 'bigint',
                         right_edge = 'bigint',
                         strand = "integer",
                         partial = "text",
                         start_type = "text",
                         rbs_motif = "text",
                         rbs_spacer = "text",
                         gc_content = "numeric",
                         length = "integer",
                         bases = "bigint",
                         coverage = "numeric",
                         reads = "bigint",
                         rpkm = "numeric",
                         frags = "bigint",
                         fpkm = "numeric",
                         tpm = "numeric")

  # # First run to create table
   # dbWriteTable(conn = pg,
   #              name = "gene_abundance",
   #              gene_abund_df,
   #              overwrite = FALSE,
   #              field.types = postgres_col_types)

  dbWriteTable(conn = pg,
               name = "gene_abundance",
               value = gene_abund_df,
               overwrite = FALSE, append = TRUE)
  
  return(NULL)
}

# Drop indices to load faster
# For the indexes created by this script
sql('DROP INDEX gene_abundance_sample') %>% DBI::dbSendQuery(pg, .)
sql('DROP INDEX gene_abundance_gene') %>%  DBI::dbSendQuery(pg, .)
sql('DROP INDEX gene_abundance_contig') %>% DBI::dbSendQuery(pg, .)

walk2(new_gene_abundance_files$path, new_gene_abundance_files$sample, read_gene_abundance,.progress = TRUE)

# Create new indices
sql('CREATE INDEX "gene_abundance_sample" ON "gene_abundance" ("sample")') %>%  DBI::dbSendQuery(pg, .)
sql('CREATE INDEX "gene_abundance_gene" ON "gene_abundance" ("gene")') %>% DBI::dbSendQuery(pg, .)
sql('CREATE INDEX "gene_abundance_contig" ON "gene_abundance" ("contig")') %>% DBI::dbSendQuery(pg, .)

```


# KOFam Scan
```{r}
kofam_scan <- tbl(pg, "kofam_scan")

kofam_scan_samples <- get_pg_table_samples(pg, "kofam_scan", "sample")

kofam_scan_files <- Sys.glob("data/omics/metagenomes/*/kofam_scan/*_kofam_results.txt") %>% 
  data.frame(path = .) %>% 
  unglue::unglue_unnest(path, "data/omics/metagenomes/{sample}/kofam_scan/{sample2}_kofam_results.txt",remove = FALSE) %>% 
  mutate(file_size = fs::file_size(path))

new_kofam_scan_files <- kofam_scan_files %>% 
  filter(!sample %in% kofam_scan_samples$sample,
         str_detect(sample, "^samp_*") | str_detect(sample, "^coassembly_*") | str_detect(sample, "^GVHDsamp_")) %>% 
  mutate(fs::file_info(path))

read_kofam_scan <- function(path, sample_name){
  
  col_name <- c("sig", "gene", "ko", "thrshld", "score","e_value", "ko_def")
  
  kofam_scan_df <- read_tsv(path,col_names = col_name,skip = 2) %>% 
    select(-ko_def) %>% 
    mutate(sig = if_else(sig == "*", TRUE, FALSE),
           sample = sample_name) %>% 
    relocate(gene, sample)
    
  # Only used when creating the table for the first time, for all subsequent additions,
  # append = TRUE should be added as the field types are already established
  postgres_col_types = c(sig = "boolean",
                         gene = "text",
                         ko = "text",
                         thrshld = "numeric",
                         score = "numeric",
                         e_value = "numeric"
                         )

  # # First run to create table
   # dbWriteTable(conn = pg,
   #              name = "kofam_scan",
   #              kofam_scan_df,
   #              overwrite = FALSE,
   #              field.types = postgres_col_types)

  dbWriteTable(conn = pg,
               name = "kofam_scan",
               value = kofam_scan_df,
               overwrite = FALSE, append = TRUE)
  
  return(NULL)
}

# Drop indices to load faster
# For the indexes created by this script
sql('DROP INDEX kofam_scan_sample') %>% DBI::dbSendQuery(pg, .)
sql('DROP INDEX kofam_scan_gene') %>%  DBI::dbSendQuery(pg, .)
sql('DROP INDEX kofam_scan_ko') %>% DBI::dbSendQuery(pg, .)
sql('DROP INDEX kofam_scan_sig') %>% DBI::dbSendQuery(pg, .)

walk2(new_kofam_scan_files$path, new_kofam_scan_files$sample, read_kofam_scan,.progress = TRUE)

# Create new indices
sql('CREATE INDEX "kofam_scan_sample" ON "kofam_scan" ("sample")') %>%  DBI::dbSendQuery(pg, .)
sql('CREATE INDEX "kofam_scan_gene" ON "kofam_scan" ("gene")') %>% DBI::dbSendQuery(pg, .)
sql('CREATE INDEX "kofam_scan_ko" ON "kofam_scan" ("ko")') %>% DBI::dbSendQuery(pg, .)
sql('CREATE INDEX "kofam_scan_sig" ON "kofam_scan" ("sig")') %>% DBI::dbSendQuery(pg, .)

```


## dRep bin abundance (within sample)
```{r}
# Get GTDB annotations that are already loaded into the database
existing_bin_abund_samples <- get_pg_table_samples(pg, "bin_abund_within_sample", "sample")

# Find all GTDB summary tsvs
drep_bin_abunds <- Sys.glob("data/omics/metagenomes/*/bins/coverage_drep_bins.tsv") %>% 
  data.frame(path = .) %>% 
  bind_cols(.,unglue::unglue_data(.$path,"data/omics/{sample_type}/{sample}/bins/coverage_drep_bins.tsv")) %>% 
  bind_cols(file.info(.$path))

# Filter list of summary tsvs to only those not already loaded into the database
new_drep_bin_abunds <- drep_bin_abunds %>% 
  filter(
      !sample %in% existing_bin_abund_samples$sample & (str_detect(sample, "^samp_") | str_detect(sample, "^coassembly_") | str_detect(sample, "^GVHDsamp_")))

# Load an example summary tsv to get the column types
abund_test <- read_tsv("data/omics/metagenomes/samp_447/bins/coverage_drep_bins.tsv") #%>% 
column_types <- readr::as.col_spec(abund_test)

# First upload to establish the table
    # postgres_col_types = c(sample = "text",
    #                        bin = "text",
    #                        percent_abund = "numeric",
    #                        mean_depth = "numeric",
    #                        trimmed_mean_depth = "numeric",
    #                        covered_bases = "bigint",
    #                        variance = "numeric",
    #                        length = "bigint",
    #                        read_count = "bigint",
    #                        reads_per_base = "numeric",
    #                        rpkm = "numeric",
    #                        tpm = "numeric"
    #                        )
    # 
    # read_tsv("data/omics/metagenomes/samp_447/bins/coverage_drep_bins.tsv") %>%
    #       mutate(sample = "samp_447") %>% 
    #       rename(bin = "Genome",
    #              percent_abund = "Relative Abundance (%)",
    #              mean_depth = "Mean",
    #              trimmed_mean_depth = "Trimmed Mean",
    #              covered_bases = "Covered Bases",
    #              variance = "Variance",
    #              length = "Length",
    #              read_count = "Read Count",
    #              reads_per_base = "Reads per base",
    #              rpkm = "RPKM",
    #              tpm = "TPM") %>% 
    #       select(-Sample) %>% 
    #       relocate(sample) %>% 
    #       dbWriteTable(pg, "bin_abund_within_sample", ., overwrite = FALSE, field.types = postgres_col_types)

# Function for reading the summary tsv and saving it to the database
read_drep_bin_abunds_to_pg <- function(path, sample) {
  print(sample)
  tryCatch({
    read_tsv(path,show_col_types = FALSE) %>%
      mutate(sample = sample) %>% 
      rename(bin = "Genome",
             percent_abund = "Relative Abundance (%)",
             mean_depth = "Mean",
             trimmed_mean_depth = "Trimmed Mean",
             covered_bases = "Covered Bases",
             variance = "Variance",
             length = "Length",
             read_count = "Read Count",
             reads_per_base = "Reads per base",
             rpkm = "RPKM",
             tpm = "TPM") %>% 
      select(-Sample) %>% 
      relocate(sample) %>% 
      dbWriteTable(pg, "bin_abund_within_sample", ., overwrite = FALSE, append = TRUE)
  }, error = function(e) {
    cat("Error occurred while processing file:", path, "\n")
    cat("Error message:", conditionMessage(e), "\n")
  })
}

#test <- read_drep_to_pg(drep_paths$path[1],sample = drep_paths$sample[1])


# Load new summaries into the database
# Drop existing indices [speeds up loading of new data]
sql('DROP INDEX public.bin_abund_within_sample_bin') %>% DBI::dbSendQuery(pg, .)
sql('DROP INDEX public.bin_abund_within_sample_sample') %>% DBI::dbSendQuery(pg, .)

# Append new data
walk2(new_drep_bin_abunds$path, new_drep_bin_abunds$sample, read_drep_bin_abunds_to_pg,.progress = TRUE)

# Recreate indices
sql('CREATE INDEX "bin_abund_within_sample_bin" ON "bin_abund_within_sample" ("bin")') %>% DBI::dbSendQuery(pg, .)
sql('CREATE INDEX "bin_abund_within_sample_sample" ON "bin_abund_within_sample" ("sample")') %>% DBI::dbSendQuery(pg, .)
```


## contig abundance (within sample)
```{r}
# Get contig abundances that are already loaded into the database
existing_contig_abund_samples <- get_pg_table_samples(pg, "contig_abund", "sample")

# Find all contig abundance tsvs
contig_abunds <- Sys.glob("data/omics/metagenomes/*/*_contig_abund.tsv") %>% 
  data.frame(path = .) %>% 
  bind_cols(.,unglue::unglue_data(.$path,"data/omics/{sample_type}/{sample}/{sample2}_contig_abund.tsv")) %>% 
  bind_cols(file.info(.$path))

# Filter list of tsvs to only those not already loaded into the database
new_contig_abunds <- contig_abunds %>% 
  filter(
      !sample %in% existing_contig_abund_samples$sample & (str_detect(sample, "^samp_") | str_detect(sample, "^coassembly_") | str_detect(sample, "^GVHDsamp_")))

# Load an example summary tsv to get the column types
abund_test <- read_tsv("data/omics/metagenomes/samp_447/samp_447_contig_abund.tsv") #%>% 
column_types <- readr::as.col_spec(drep_test)

# First upload to establish the table
    # postgres_col_types = c(sample = "text",
    #                        contig = "text",
    #                        mean_depth = "numeric",
    #                        trimmed_mean_depth = "numeric",
    #                        covered_bases = "bigint",
    #                        variance = "numeric",
    #                        length = "bigint",
    #                        read_count = "bigint",
    #                        reads_per_base = "numeric",
    #                        rpkm = "numeric",
    #                        tpm = "numeric"
    #                        )
    # 
    # read_tsv("data/omics/metagenomes/samp_447/samp_447_contig_abund.tsv") %>%
    #       mutate(sample = "samp_447") %>%
    #       rename(contig = "Contig",
    #              mean_depth = "Mean",
    #              trimmed_mean_depth = "Trimmed Mean",
    #              covered_bases = "Covered Bases",
    #              variance = "Variance",
    #              length = "Length",
    #              read_count = "Read Count",
    #              reads_per_base = "Reads per base",
    #              rpkm = "RPKM",
    #              tpm = "TPM") %>%
    #       select(-Sample) %>%
    #       relocate(sample) %>%
    #       dbWriteTable(pg, "contig_abund", ., overwrite = FALSE, field.types = postgres_col_types)

# Function for reading the summary tsv and saving it to the database
read_contig_abunds_to_pg <- function(path, sample) {
  print(sample)
  tryCatch({
    read_tsv(path,show_col_types = FALSE) %>%
      mutate(sample = sample) %>% 
      rename(contig = "Contig",
             mean_depth = "Mean",
             trimmed_mean_depth = "Trimmed Mean",
             covered_bases = "Covered Bases",
             variance = "Variance",
             length = "Length",
             read_count = "Read Count",
             reads_per_base = "Reads per base",
             rpkm = "RPKM",
             tpm = "TPM") %>%
      select(-Sample) %>% 
      relocate(sample) %>% 
      dbWriteTable(pg, "contig_abund", ., overwrite = FALSE, append = TRUE)
  }, error = function(e) {
    cat("Error occurred while processing file:", path, "\n")
    cat("Error message:", conditionMessage(e), "\n")
  })
}

#test <- read_drep_to_pg(drep_paths$path[1],sample = drep_paths$sample[1])


# Load new summaries into the database
# Drop existing indices [speeds up loading of new data]
sql('DROP INDEX public.contig_abund_contig') %>% DBI::dbSendQuery(pg, .)
sql('DROP INDEX public.contig_abund_sample') %>% DBI::dbSendQuery(pg, .)

# Append new data
walk2(new_contig_abunds$path, new_contig_abunds$sample, read_contig_abunds_to_pg,.progress = TRUE)

# Recreate indices
sql('CREATE INDEX "contig_abund_contig" ON "contig_abund" ("contig")') %>% DBI::dbSendQuery(pg, .)
sql('CREATE INDEX "contig_abund_sample" ON "contig_abund" ("sample")') %>% DBI::dbSendQuery(pg, .)
```


## Microcystis marker gene (per sequence)
```{r}
# Get contig abundances that are already loaded into the database
existing_mc_marker_abund <- get_pg_table_samples(pg, "mc_marker_abunds", "sample")

# Find all contig abundance tsvs
mc_marker_abunds <- Sys.glob("data/omics/*/*/microcystis_markers/*--*_summary.tsv") %>% 
  data.frame(path = .) %>% 
  bind_cols(.,unglue::unglue_data(.$path,"data/omics/{sample_type}/{sample}/microcystis_markers/{sample2}--{marker}_summary.tsv")) %>% 
  bind_cols(file.info(.$path))

# Filter list of tsvs to only those not already loaded into the database
new_mc_marker_abunds <- mc_marker_abunds %>% 
  filter(
      !sample %in% existing_mc_marker_abund$sample & (str_detect(sample, "^samp_") | str_detect(sample, "^coassembly_")))

# Load an example summary tsv to get the column types
#abund_test <- read_tsv("data/omics/metagenomes/samp_447/microcystis_markers/samp_447--lgt__516_summary.tsv") #%>% 
#column_types <- as.col_spec(drep_test)

# # First upload to establish the table
# postgres_col_types = c(sample = "text",
#                        marker = "text",
#                        seqnames = "text",
#                        seqlength = "integer",
#                        mapped = "integer",
#                        sample_read_count = "bigint",
#                        rpkm = "numeric",
#                        short_name = "text",
#                        clade = "text"
#                        )
# 
# read_tsv("data/omics/metagenomes/samp_447/microcystis_markers/samp_447--lgt__516_summary.tsv",show_col_types = FALSE) %>%
#       select(seqnames,seqlength,mapped,sample_read_count,rpkm,short_name,clade) %>%
#       mutate(sample = "samp_447",
#              marker = "lgt__516") %>%
#       relocate(sample, marker) %>%
#       dbWriteTable(pg, "mc_marker_abunds", ., overwrite = FALSE, field.types = postgres_col_types)

# Function for reading the summary tsv and saving it to the database
read_mc_marker_abunds_to_pg <- function(path, sample, marker) {
  #print(sample)
  tryCatch({
    read_tsv(path, show_col_types = FALSE) %>%
      mutate(sample = sample,
             marker = marker) %>% 
      select(sample, marker, seqnames,seqlength,mapped,sample_read_count,rpkm,short_name,clade) %>%
      dbWriteTable(pg, "mc_marker_abunds", ., overwrite = FALSE, append = TRUE)
  }, error = function(e) {
    cat("Error occurred while processing file:", path, "\n")
    cat("Error message:", conditionMessage(e), "\n")
  })
}

#test <- read_drep_to_pg(drep_paths$path[1],sample = drep_paths$sample[1])


# Load new summaries into the database
# Drop existing indices [speeds up loading of new data]
sql('DROP INDEX public.mc_marker_abunds_sample') %>% DBI::dbSendQuery(pg, .)
sql('DROP INDEX public.mc_marker_abunds_seq') %>% DBI::dbSendQuery(pg, .)

# Append new data
pwalk(list(path = new_mc_marker_abunds$path, 
           sample = new_mc_marker_abunds$sample, 
           marker = new_mc_marker_abunds$marker),
          read_mc_marker_abunds_to_pg,.progress = TRUE)

# Recreate indices
sql('CREATE INDEX "mc_marker_abunds_sample" ON "mc_marker_abunds" ("sample")') %>% DBI::dbSendQuery(pg, .)
sql('CREATE INDEX "mc_marker_abunds_seq" ON "mc_marker_abunds" ("seqnames")') %>% DBI::dbSendQuery(pg, .)
```

## Microcystis marker gene (clade level summary)
```{r}
# Get contig abundances that are already loaded into the database
existing_mc_clade_abund <- get_pg_table_samples(pg, "mc_clade_abunds", "sample")

# Find all contig abundance tsvs
mc_clade_abunds <- Sys.glob("data/omics/*/*/microcystis_markers/*--*_clade-summary.tsv") %>% 
  data.frame(path = .) %>% 
  bind_cols(.,unglue::unglue_data(.$path,"data/omics/{sample_type}/{sample}/microcystis_markers/{sample2}--{marker}_clade-summary.tsv")) %>% 
  bind_cols(file.info(.$path))

# Filter list of tsvs to only those not already loaded into the database
new_mc_clade_abunds <- mc_clade_abunds %>% 
  filter(
      !sample %in% existing_mc_clade_abund$sample & (str_detect(sample, "^samp_") | str_detect(sample, "^coassembly_")))

# Load an example summary tsv to get the column types
abund_test <- read_tsv("data/omics/metagenomes/samp_447/microcystis_markers/samp_447--lgt__516_clade-summary.tsv",show_col_types = FALSE) #%>% 


# # First upload to establish the table
# postgres_col_types = c(sample = "text",
#                        marker = "text",
#                        clade = "text",
#                        rpkm = "numeric",
#                        mapped_reads = "integer"
#                        )
# 
# read_tsv("data/omics/metagenomes/samp_447/microcystis_markers/samp_447--lgt__516_clade-summary.tsv",show_col_types = FALSE) %>%
#       select(clade, rpkm, mapped_reads) %>% 
#       mutate(sample = "samp_447",
#              marker = "lgt__516") %>%
#       relocate(sample, marker) %>%
#       dbWriteTable(pg, "mc_clade_abunds", ., overwrite = FALSE, field.types = postgres_col_types)

# Function for reading the summary tsv and saving it to the database
read_mc_clade_abunds_to_pg <- function(path, sample, marker) {
  #print(sample)
  tryCatch({
    read_tsv(path, show_col_types = FALSE) %>%
      mutate(sample = sample,
             marker = marker) %>% 
      select(sample, marker, clade, rpkm, mapped_reads) %>% 
      dbWriteTable(pg, "mc_clade_abunds", ., overwrite = FALSE, append = TRUE)
  }, error = function(e) {
    cat("Error occurred while processing file:", path, "\n")
    cat("Error message:", conditionMessage(e), "\n")
  })
}

#test <- read_drep_to_pg(drep_paths$path[1],sample = drep_paths$sample[1])


# Load new summaries into the database
# Drop existing indices [speeds up loading of new data]
sql('DROP INDEX public.mc_clade_abund_sample') %>% DBI::dbSendQuery(pg, .)
sql('DROP INDEX public.mc_clade_abund_clade') %>% DBI::dbSendQuery(pg, .)

# Append new data
pwalk(list(path = new_mc_clade_abunds$path, 
           sample = new_mc_clade_abunds$sample, 
           marker = new_mc_clade_abunds$marker),
          read_mc_clade_abunds_to_pg,.progress = TRUE)

# Recreate indices
sql('CREATE INDEX "mc_clade_abund_sample" ON "mc_clade_abunds" ("sample")') %>% DBI::dbSendQuery(pg, .)
sql('CREATE INDEX "mc_clade_abund_clade" ON "mc_clade_abunds" ("clade")') %>% DBI::dbSendQuery(pg, .)
```


## Amplicon detection summary 
```{r}
# Get contig abundances that are already loaded into the database
existing_amplicon_summaries <- tbl(pg, "amplicon_region_summary") %>% 
  select(sample, read_direction) %>% 
  distinct() %>% 
  collect()

# Find all contig abundance tsvs
amplicon_summaries <- Sys.glob("data/omics/amplicons/*/detect_region/*_summary.tsv") %>% 
  data.frame(path = .) %>% 
  bind_cols(.,unglue::unglue_data(.$path,"data/omics/{sample_type}/{sample}/detect_region/{read_direction}_summary.tsv")) %>% 
  bind_cols(file.info(.$path))

# Filter list of tsvs to only those not already loaded into the database
new_amplicon_summaries <- amplicon_summaries %>% 
  anti_join(existing_amplicon_summaries) %>% 
  filter(
       (str_detect(sample, "^samp_") | str_detect(sample, "^coassembly_"))
       )

# Load an example summary tsv to get the column types
amplicon_summary_test <- read_tsv("data/omics/amplicons/samp_100/detect_region/fwd_summary.tsv",show_col_types = FALSE) #%>% 


# # First upload to establish the table
# postgres_col_types = c(sample = "text",
#                        read_direction = "text",
#                        hmm_model = "text",
#                        n_seqs = "integer",
#                        hmm_start_median = "numeric",
#                        hmm_end_median = "numeric",
#                        e_value_median = "numeric",
#                        score_median = "numeric",
#                        seq_start_median = "numeric",
#                        seq_end_median = "numeric",
#                        tax_group = "text",
#                        gene = "text"
#                        )
# 
# read_tsv("data/omics/amplicons/samp_100/detect_region/fwd_summary.tsv",show_col_types = FALSE) %>%
#       mutate(sample = "samp_100",
#              read_direction = "fwd") %>%
#       relocate(sample) %>%
#       dbWriteTable(pg, "amplicon_region_summary", ., overwrite = TRUE, field.types = postgres_col_types)

# Function for reading the summary tsv and saving it to the database
read_amplicon_summaries_to_pg <- function(path, sample, direction) {
  #print(sample)
  tryCatch({
    read_tsv(path, show_col_types = FALSE) %>%
      mutate(sample = sample,
             read_direction = direction) %>% 
      relocate(sample) %>%
      dbWriteTable(pg, "amplicon_region_summary", ., overwrite = FALSE, append = TRUE)
  }, error = function(e) {
    cat("Error occurred while processing file:", path, "\n")
    cat("Error message:", conditionMessage(e), "\n")
  })
}

#test <- read_drep_to_pg(drep_paths$path[1],sample = drep_paths$sample[1])


# Load new summaries into the database
# Drop existing indices [speeds up loading of new data]
#sql('DROP INDEX public.amplicon_region_summary_sample') %>% DBI::dbSendQuery(pg, .)
#sql('DROP INDEX public.amplicon_region_summary_direction') %>% DBI::dbSendQuery(pg, .)

# Append new data
pwalk(list(path = new_amplicon_summaries$path, 
           sample = new_amplicon_summaries$sample, 
           direction = new_amplicon_summaries$read_direction),
          read_amplicon_summaries_to_pg,.progress = TRUE)

# Recreate indices
sql('CREATE INDEX "amplicon_region_summary_sample" ON "amplicon_region_summary" ("sample")') %>% DBI::dbSendQuery(pg, .)
sql('CREATE INDEX "amplicon_region_summary_direction" ON "amplicon_region_summary" ("read_direction")') %>% DBI::dbSendQuery(pg, .)
```


# Tax abund summary from contig LCA
```{r}

"data/omics/metagenomes/samp_447/samp_447_lca_abund_summarized.tsv"

# Get GTDB annotations that are already loaded into the database
existing_tax_abund_from_contig_lca_and_abund_samples <- get_pg_table_samples(pg, "tax_abund_from_contig_lca_and_abund", "sample")

# Find all GTDB summary tsvs
tax_abund_from_contig_lca_and_abund_tsvs <- Sys.glob("data/omics/metagenomes/*/*_lca_abund_summarized.tsv") %>% 
  data.frame(path = .) %>% 
  bind_cols(.,unglue::unglue_data(.$path,"data/omics/{sample_type}/{sample}/{sample2}_lca_abund_summarized.tsv")) %>% 
  bind_cols(file.info(.$path))

# Filter list of summary tsvs to only those not already loaded into the database
new_existing_tax_abund_from_contig_lca_and_abunds <- tax_abund_from_contig_lca_and_abund_tsvs %>% 
  filter(
      !sample %in% existing_tax_abund_from_contig_lca_and_abund_samples$sample & (str_detect(sample, "^samp_") | str_detect(sample, "^coassembly_") | str_detect(sample, "^GVHDsamp_")))

# Load an example summary tsv to get the column types
abund_test <- read_tsv("data/omics/metagenomes/samp_447/samp_447_lca_abund_summarized.tsv") %>% mutate(sample = "samp_447")
column_types <- as.col_spec(abund_test)

# First upload to establish the table
    # postgres_col_types = c(tax_id = "integer",
    #                        abund_w_subtax = "numeric",
    #                        abund_direct = "numeric",
    #                        sample = "text"
    #                        )
    # 
    # read_tsv("data/omics/metagenomes/samp_447/samp_447_lca_abund_summarized.tsv") %>%
    #       mutate(sample = "samp_447") %>%
    #       relocate(sample) %>%
    #       dbWriteTable(pg, "tax_abund_from_contig_lca_and_abund", ., overwrite = FALSE, field.types = postgres_col_types)

# Function for reading the summary tsv and saving it to the database
read_tax_abund_from_contig_lca_and_abund_to_pg <- function(path, sample) {
  print(sample)
  tryCatch({
    read_tsv(path) %>%
      mutate(sample = sample) %>% 
      relocate(sample) %>% 
      dbWriteTable(pg, "tax_abund_from_contig_lca_and_abund", ., overwrite = FALSE, append = TRUE)
  }, error = function(e) {
    cat("Error occurred while processing file:", path, "\n")
    cat("Error message:", conditionMessage(e), "\n")
  })
}

#test <- read_drep_to_pg(drep_paths$path[1],sample = drep_paths$sample[1])


# Load new summaries into the database
# Drop existing indices [speeds up loading of new data]
sql('DROP INDEX public.tax_abund_from_contig_lca_and_abund_tax_id') %>% DBI::dbSendQuery(pg, .)
sql('DROP INDEX public.tax_abund_from_contig_lca_and_abund_sample') %>% DBI::dbSendQuery(pg, .)

# Append new data
walk2(new_existing_tax_abund_from_contig_lca_and_abunds$path, 
      new_existing_tax_abund_from_contig_lca_and_abunds$sample, 
      read_tax_abund_from_contig_lca_and_abund_to_pg,
      .progress = TRUE)

# Recreate indices
sql('CREATE INDEX "tax_abund_from_contig_lca_and_abund_tax_id" ON "tax_abund_from_contig_lca_and_abund" ("tax_id")') %>% DBI::dbSendQuery(pg, .)
sql('CREATE INDEX "tax_abund_from_contig_lca_and_abund_sample" ON "tax_abund_from_contig_lca_and_abund" ("sample")') %>% DBI::dbSendQuery(pg, .)
```


# Benchmarks
```{r}
# Get benchmarks that are already loaded into the database
existing_benchmark_samples <- tbl(pg,"benchmarks") %>% 
  dplyr::select(sample, rule) %>% distinct() %>% 
  collect()

# Find all benchmark tsvs
benchmark_txts <- Sys.glob("benchmarks/*/*.txt") %>% 
  data.frame(path = .) %>% 
  bind_cols(.,unglue::unglue_data(.$path,"benchmarks/{rule}/{filename}.txt")) %>% 
  mutate(sample = str_extract(filename, "(samp[_-]\\d+|GVHDsamp[_-]\\d+|coassembly[_-]\\d+)"),
         sample = str_replace(sample, "-","_")) %>% 
  bind_cols(file.info(.$path))

benchmark_tsvs <- Sys.glob("benchmarks/*/*.tsv") %>% 
  data.frame(path = .) %>% 
  bind_cols(.,unglue::unglue_data(.$path,"benchmarks/{rule}/{filename}.tsv")) %>% 
  mutate(sample = str_extract(filename, "(samp[_-]\\d+|GVHDsamp[_-]\\d+|coassembly[_-]\\d+)"),
         sample = str_replace(sample, "-","_")) %>% 
  bind_cols(file.info(.$path))

benchmarks <- bind_rows(benchmark_txts, benchmark_tsvs)

# Filter list of summary tsvs to only those not already loaded into the database
new_benchmark_tsvs <- benchmarks %>% 
  anti_join(existing_benchmark_samples) %>% 
  filter(str_detect(sample, "^samp_") | str_detect(sample, "^coassembly_") | str_detect(sample, "^GVHDsamp_"))

# Load an example summary tsv to get the column types


# First upload to establish the table

    # benchmark <- read_tsv("benchmarks/concoct/metagenomes-GVHDset2__GVHDsamp_306.txt") %>% 
    #   janitor::clean_names() %>% 
    #   mutate(sample = "GVHDsamp_306", 
    #          rule = "concoct", 
    #          h_m_s = as.character(h_m_s),
    #          mod_time = fs::file_info("benchmarks/concoct/metagenomes-GVHDset2__GVHDsamp_306.txt") %>% pull(modification_time))
    # 
    # column_types <- as.col_spec(benchmark)
    # 
    # # postgres_col_types = c(tax_id = "integer",
    # #                        abund_w_subtax = "numeric",
    # #                        abund_direct = "numeric",
    # #                        sample = "text"
    # #                        )
    # 
    # benchmark %>%
    #       relocate(sample,rule) %>%
    #       dbWriteTable(pg, "benchmarks", ., overwrite = FALSE)

# Function for reading the summary tsv and saving it to the database
read_benchmarks_to_pg <- function(path, sample, rule) {
  print(sample)
  tryCatch({
    read_tsv(path,show_col_types = FALSE) %>%
      janitor::clean_names() %>% 
      mutate(sample = sample, 
             rule = rule, 
             h_m_s = as.character(h_m_s),
             mod_time = fs::file_info(path) %>% 
               pull(modification_time)) %>% 
      dplyr::relocate(sample,rule) %>% 
      dbWriteTable(pg, "benchmarks", ., overwrite = FALSE, append = TRUE)
  }, error = function(e) {
    cat("Error occurred while processing file:", path, "\n")
    cat("Error message:", conditionMessage(e), "\n")
  })
}

# Load new summaries into the database
# Drop existing indices [speeds up loading of new data]
sql('DROP INDEX public.benchmarks_sample') %>% DBI::dbSendQuery(pg, .)
sql('DROP INDEX public.benchmarks_rule') %>% DBI::dbSendQuery(pg, .)

pg <- DBI::dbConnect(RPostgres::Postgres(),dbname = "glamr_data", host = "cayman.earth.lsa.umich.edu", port = "5432", user = "glamr_admin", password = "glamr2023")
# Append new data
purrr::pwalk(.l = list(new_benchmark_tsvs$path,
                  new_benchmark_tsvs$sample,
                  new_benchmark_tsvs$rule),
             .f = read_benchmarks_to_pg,.progress = TRUE)

# Recreate indices
sql('CREATE INDEX "benchmarks_sample" ON "benchmarks" ("sample")') %>% DBI::dbSendQuery(pg, .)
sql('CREATE INDEX "benchmarks_rule" ON "benchmarks" ("rule")') %>% DBI::dbSendQuery(pg, .)
```



# BLASTn-like searches of contigs
```{r}
# Get results already loaded into the database
existing_contig_search_res <- tbl(pg, "contig_blast") %>% 
  select(sample,search_name) %>% 
  distinct() %>% 
  collect()

# Find all result files
contig_search_res <- Sys.glob("data/omics/metagenomes/*/contig_search/*.m8") %>% 
  data.frame(path = .) %>% 
  bind_cols(.,unglue::unglue_data(.$path,"data/omics/{sample_type}/{sample}/contig_search/{query}.m8")) %>% 
  bind_cols(file.info(.$path))

# Filter list of summary tsvs to only those not already loaded into the database
new_contig_search_res <- contig_search_res %>% 
  anti_join(existing_contig_search_res %>% rename(query = "search_name")) %>% 
  filter(str_detect(sample, "^samp_") | str_detect(sample, "^coassembly_") | str_detect(sample, "^GVHDsamp_"))

# Load an example summary tsv to get the column types
# col_names_m8 <- c("query", "subject","percent_id", "alignment_length", "mismatches", "gap_openings", "query_start", "query_end", "subject_start", "subject_end", "e_value","bit_score")
# 
# contig_search_test <- read_tsv("data/omics/metagenomes/samp_2233/contig_search/guanitoxin.m8",col_names = col_names_m8) %>% mutate(sample = "samp_2233", search_name = "guanitoxin")
# column_types <- as.col_spec(contig_search_test)

# First upload to establish the table
    # postgres_col_types = c(query = "text", 
    #                        subject = "text",
    #                        percent_id = "numeric", 
    #                        alignment_length = "integer", 
    #                        mismatches = "integer", 
    #                        gap_openings = "integer", 
    #                        query_start = "bigint", 
    #                        query_end = "bigint", 
    #                        subject_start = "bigint", 
    #                        subject_end = "bigint", 
    #                        e_value = "numeric",
    #                        bit_score = "numeric"
    #                        )
    # 
    # read_tsv("data/omics/metagenomes/samp_2233/contig_search/guanitoxin.m8",col_names = col_names_m8) %>% 
    #   mutate(sample = "samp_2233", search_name = "guanitoxin") %>% 
    #   relocate(sample, search_name) %>% 
    #   dbWriteTable(pg, "contig_blast", ., overwrite = FALSE, field.types = postgres_col_types)

# Function for reading the summary tsv and saving it to the database
read_contig_search_res <- function(path, sample, search_name) {
  col_names_m8 <- c("query", "subject","percent_id", "alignment_length", "mismatches", "gap_openings", "query_start", "query_end", "subject_start", "subject_end", "e_value","bit_score")
  print(sample)
  tryCatch({
    read_tsv(path,col_names = col_names_m8,show_col_types = FALSE) %>%
      mutate(sample = sample, search_name = search_name) %>%
      relocate(sample, search_name) %>%
      dbWriteTable(pg, "contig_blast", ., overwrite = FALSE, append = TRUE)
  }, error = function(e) {
    cat("Error occurred while processing file:", path, "\n")
    cat("Error message:", conditionMessage(e), "\n")
  })
}

#test <- read_drep_to_pg(drep_paths$path[1],sample = drep_paths$sample[1])


# Load new summaries into the database
# Drop existing indices [speeds up loading of new data]
sql('DROP INDEX public.contig_blast_sample') %>% DBI::dbSendQuery(pg, .)
sql('DROP INDEX public.contig_blast_search_name') %>% DBI::dbSendQuery(pg, .)

# Append new data
pwalk(list(path = new_contig_search_res$path, 
           sample = new_contig_search_res$sample, 
           search_name = new_contig_search_res$query),
      read_contig_search_res,
      .progress = TRUE)

# Recreate indices
sql('CREATE INDEX "contig_blast_sample" ON "contig_blast" ("sample")') %>% DBI::dbSendQuery(pg, .)
sql('CREATE INDEX "contig_blast_search_name" ON "contig_blast" ("search_name")') %>% DBI::dbSendQuery(pg, .)
```



#### Antismash assembly
##### Counts
```{r}
antismash_assembly_pg <- tbl(pg, "antismash_assembly_counts")

antismash_assembly_samples <- get_pg_table_samples(pg, "antismash_assembly_counts", "sample")

antismash_assembly_files <- Sys.glob("data/omics/metagenomes/*/antismash8_assembly/summaries/counts.tsv") %>% 
  data.frame(path = .) %>% 
  unglue::unglue_unnest(path, "data/omics/{sample_type}/{sample}/antismash8_assembly/summaries/counts.tsv",remove = FALSE)

new_antismash_assembly_files <- antismash_assembly_files %>% 
  filter(!sample %in% antismash_assembly_samples$sample,
         str_detect(sample, "^samp_*") | str_detect(sample, "^coassembly_") | str_detect(sample, "^GVHDsamp_")) %>% 
  mutate(fs::file_info(path))

#path = new_antismash_assembly_files$path[1]
#sample_name = new_antismash_assembly_files$sample[1]
read_antismash_counts <- function(path, sample_name){
  
  counts_df <- read_tsv(path,comment = "#") %>% 
    mutate(sample = sample_name) %>% 
    pivot_longer(-c(sample, record, total_count, description), names_to = "group", values_to = "count") %>% 
    select(sample,group, count)
  
  
  # # Only used when creating the table for the first time, for all subsequent additions,
  # # append = TRUE should be added as the field types are already established
  # postgres_col_types = c(sample = 'text',
  #                        group = "text",
  #                        count = 'integer')
  # 
  # #First run to create table
  # dbWriteTable(pg,"antismash_assembly_counts",counts_df,overwrite = FALSE, field.types = postgres_col_types)

  dbWriteTable(pg,"antismash_assembly_counts",counts_df,overwrite = FALSE, append = TRUE)
  
  return(NULL)
}

# For the indexes created by this script
sql('DROP INDEX public.antismash_assembly_counts_sample') %>% DBI::dbSendQuery(pg, .)
sql('DROP INDEX public.antismash_assembly_counts_group') %>% DBI::dbSendQuery(pg, .)

walk2(new_antismash_assembly_files$path, new_antismash_assembly_files$sample, read_antismash_counts,.progress = TRUE)

sql('CREATE INDEX "antismash_assembly_counts_sample" ON "antismash_assembly_counts" ("sample")') %>% DBI::dbSendQuery(pg, .)
sql('CREATE INDEX "antismash_assembly_counts_group" ON "antismash_assembly_counts" ("group")') %>%  DBI::dbSendQuery(pg, .)
```

##### regions
```{r}
antismash_assembly_regions_pg <- tbl(pg, "antismash_assembly_regions")

antismash_assembly_samples <- get_pg_table_samples(pg, "antismash_assembly_regions", "sample")

antismash_assembly_files <- Sys.glob("data/omics/metagenomes/*/antismash8_assembly/summaries/region_summary.tsv") %>% 
  data.frame(path = .) %>% 
  unglue::unglue_unnest(path, "data/omics/{sample_type}/{sample}/antismash8_assembly/summaries/region_summary.tsv",remove = FALSE)

new_antismash_assembly_files <- antismash_assembly_files %>% 
  ungroup() %>% 
  filter(!sample %in% antismash_assembly_samples$sample,
         str_detect(sample, "^samp_*") | str_detect(sample, "^coassembly_") | str_detect(sample, "^GVHDsamp_")) %>% 
  mutate(fs::file_info(path))

# path = new_antismash_assembly_files$path[1]
# sample_name = new_antismash_assembly_files$sample[1]
read_antismash_regions <- function(path, sample_name){
  
  regions_df <- read_tsv(path,comment = "#") %>% 
    mutate(sample = sample_name) %>% 
    relocate(sample) %>% 
      dplyr::select(-file, -record_desc, contig = "record_id")
  
  
  # # Only used when creating the table for the first time, for all subsequent additions,
  # # append = TRUE should be added as the field types are already established
  # postgres_col_types = c(sample = 'text',
  #                        contig = "text",
  #                        region = "integer",
  #                        start = "bigint",
  #                        end = "bigint",
  #                        contig_edge = "boolean",
  #                        product = "text",
  #                        KCB_hit = "text",
  #                        KCB_acc = "text",
  #                        KCB_sim = "text")
  # 
  # #First run to create table
  # dbWriteTable(pg,"antismash_assembly_regions",regions_df,overwrite = FALSE, field.types = postgres_col_types)

  dbWriteTable(pg,"antismash_assembly_regions",regions_df,overwrite = FALSE, append = TRUE)
  
  return(NULL)
}

# For the indexes created by this script
sql('DROP INDEX public.antismash_assembly_regions_sample') %>% DBI::dbSendQuery(pg, .)
sql('DROP INDEX public.antismash_assembly_regions_contig') %>% DBI::dbSendQuery(pg, .)

walk2(new_antismash_assembly_files$path, new_antismash_assembly_files$sample, read_antismash_regions,.progress = TRUE)

sql('CREATE INDEX "antismash_assembly_regions_sample" ON "antismash_assembly_regions" ("sample")') %>% DBI::dbSendQuery(pg, .)
sql('CREATE INDEX "antismash_assembly_regions_contig" ON "antismash_assembly_regions" ("contig")') %>%  DBI::dbSendQuery(pg, .)
```


