#!/usr/bin/env python3
import argparse
from collections import Counter
from itertools import groupby

from amplicon import get_models
from amplicon.alignment import HMMRAlignment


WIGGLE_ROOM = 2


argp = argparse.ArgumentParser()
argp.add_argument('infile')
args = argp.parse_args()


models = get_models()

# 1. load alignment data
rows = []
low_quality_count = 0
with open(args.infile) as ifile:
    for lnum, line in enumerate(ifile, start=1):
        if line.startswith('#'):
            # two header lines and some trailing comments
            continue
        row = line.strip().split()
        if row[0] not in models:
            argp.error(f'unknown HMM model: "{row[0]}" at line {lnum}')
        try:
            row = HMMRAlignment(
                model=models[row[0]],
                qname=row[2],
                hmmfrom=int(row[4]),
                hmmto=int(row[5]),
                envfrom=int(row[8]),
                envto=int(row[9]),
                strand=row[11],
                score=float(row[13]),
            )
        except (KeyError, ValueError) as e:
            argp.error(f'failed parsing line {lnum}: {e}')
        rows.append(row)
    print(f'# total alignments: {len(rows)}')

if not rows:
    # nothing else to be done
    argp.exit()

# 2. pick a winner per read
winners = []
tie_count = 0
for _, grp in groupby(rows, key=lambda x: x.qname):
    top = None
    highscore = -999999999999
    tiescore = None
    for row in grp:
        if row.score > highscore:
            top = row
            highscore = row.score
        elif row.score == highscore:
            tiescore = row.score
    if tiescore is not None and tiescore == highscore:
        # top scores are tied, discard this
        tie_count += 1
        continue
    winners.append(top)
if tie_count:
    print(f'# discarding due to tied top hit score: {tie_count}')
print(f'# keeping top hits per read: {len(winners)}')
for name, count in Counter(i.model.name for i in winners).most_common():
    print(f'  #+ {name}: {count:>4}')

# 3. pass smell test
good_hits = [i for i in winners if i.pass_test]
print(f'# discarded for low quality: {len(winners) - len(good_hits)}')
if not good_hits:
    argp.exit()

# 4. find matching primer
for i in good_hits:
    i.primer_score()

hits = [i for i in good_hits if hasattr(i, 'nearest_primer')]

# 5. compile a summary


def by_primer(hit):
    """ sort/group key """
    if hit.nearest_primer is None:
        return ('', False)
    else:
        return (hit.nearest_primer, hit.has_primer)


hits = sorted(hits, key=by_primer)
major_group = None
for (primer, has_primer), grp in groupby(hits, key=by_primer):
    grp = list(grp)

    if primer:
        if major_group is None or len(grp) > len(major_group):
            major_group = grp
            top_model = grp[0].model
            top_primer = grp[0].nearest_primer
            top_has_primer = grp[0].has_primer
            top_direction = grp[0].direction
    else:
        model_names = ', '.join(sorted(set(i.model.name for i in grp)))
        print(f'# primer info missing [{model_names}]: {len(grp)}')

if major_group is None:
    argp.error('none of the models with hits have primer info')

scores = Counter(i.primer_score for i in major_group).most_common()
topscore = scores[0][0]
if len(scores) > 1 and scores[1][0] == topscore:
    # need to implement this case
    raise RuntimeError('no unique top score')

consensus_count = 0
outlier_count = 0
for score, count in scores:
    if (topscore - WIGGLE_ROOM) <= score <= (topscore + WIGGLE_ROOM):
        consensus_count += count
    else:
        outlier_count += count

print(f'model: {top_model}')
print(f'direction: {top_direction}')
print(f'primer: {top_primer}')
print(f'primer_content: {"yes" if top_has_primer else "no"}')
print(f'distance: {topscore}')
print(f'count: {consensus_count}')
print(f'outlier_count: {outlier_count}')
