#!/usr/bin/env python3
"""
Run DADA2 on amplicon sequences of given dataset to obtain ASVs and abundance
"""
import argparse
from collections import Counter
from datetime import datetime
from pathlib import Path
import re
from subprocess import CalledProcessError, run
import sys
from tempfile import TemporaryDirectory

SINGLE_MODE_THRESHOLT = 0.8
""" If there is only a single assigned target (besides UNKNOWN or NO_INFO) and
the proportion of samples with this definite target is larger than this
thresholt, then all samples will be automatically assigned to this (mode)
target. """

GUESS_TARGET_SCRIPT = './guess-amplicon-target'
""" helper script, path relative to this script """

SUMMARIZE_HMMSCAN_SCRIPT = './summarize_hmmscan.R'
""" helper script, path relative to this script """

DEFAULT_OUT_TARGETS = 'amplicon_target_assignments.tsv'
DEFAULT_OUT_SAMPLES = 'sample_files.tsv'

UNKNOWN = 'UNKNOWN'
NO_INFO = 'NO_INFO'

argp = argparse.ArgumentParser(description=__doc__)
argp.add_argument(
    'project_directory',
    help='The project directory, usually data/projects/<dataset> where the '
         'last component of the given path is interpreted as the dataset ID.  '
         'This directory is expected to contain an "amplicon" subdirectory '
         'that in turn contains one subdirectory per sample.',
)
argp.add_argument(
    '--test-mode',
    action='store_true',
    help='Run script in test mode.  Normally this script depends on '
         'target_info.txt files present for each samples.  In test mode the '
         'script will try to run the necessary machinery to (temporarily) '
         'generate such files as needed.',
)
argp.add_argument(
    '--use-import-log',
    action='store_true',
    help='Get info on a dataset\'s samples from the import log.  This also '
    'checks that raw read files, etc. actually exist.  The default is to just '
    'look at the filesystem and does not check for file existence.  The '
    'snakemake run amplicon pipeline should use the default.',
)
argp.add_argument(
    '--out-targets',
    metavar='<path>',
    default=DEFAULT_OUT_TARGETS,
    help='Path to output sample listing file.  This will a tab-separated table'
         ' listing samples and their amplicon targets.  The file should '
         'normally be reviewed and amended for correctness and completeness.'
         f'Defaults to "{DEFAULT_OUT_TARGETS}"',
)
argp.add_argument(
    '--out-samples',
    metavar='<path>',
    default='./sample_files.tsv',
    help='Path to output sample listing file.  This will be a tab-separated '
         'table listing samples with their paths to raw reads.  '
         f'Defaults to "{DEFAULT_OUT_SAMPLES}"',
)
argp.add_argument(
    '--out-details',
    metavar='<path>',
    default=None,
    help='Path to output detailed amplicon target information file.'
)
args = argp.parse_args()


# assuming proj dir is like data/project/set_xx and data/import_logs exists
proj_dir = Path(args.project_directory)
if not proj_dir.is_dir():
    argp.error(f'no such directory: {proj_dir}')
if proj_dir.parts[-3:-1] != ('data', 'projects'):
    # not like path/to/data/projects/set_nn
    print('[WARNING] non-standard project directory', file=sys.stderr)
root = proj_dir.parent.parent.parent  # the GLAMR root directory


NO_TARGET_INFO = object()


def get_sample_info_log():
    """ collect info from omics pipeline import log """
    import_log_dir = Path(args.project_directory).parent.parent / 'import_logs'
    try:
        import_log = sorted(import_log_dir.glob('*_sample_status.tsv'))[-1]
    except Exception as e:
        argp.error(f'Failed finding import log: {e.__class__.__name__}: {e}')

    data = []
    with open(import_log) as ifile:
        print(f'Reading import log: {import_log}... ', end='', flush=True)
        header = ifile.readline().strip().split('\t')
        for line in ifile:
            row = dict(zip(header, line.strip().split('\t')))

            if not row['StudyID'] == args.dataset:
                continue

            if not row['sample_type'] == 'amplicon':
                continue

            if not row['import_success'] == 'TRUE':
                continue

            # check sample dir
            try:
                sample_dir = Path(row['sample_dir'])
            except ValueError as e:
                argp.error(f'failed parsing sample_dir: {e}\n{row=}')

            # get paths to fastq files
            fwd = Path(row['raw_reads_fp'])
            try:
                fwd = root / fwd
            except ValueError as e:
                argp.error(f'unexpected path: {e}\n{row=}')

            if not fwd.is_file():
                argp.error(f'forward fastq file does not exist: {fwd}')

            rev = fwd.parent / (fwd.name.replace('fwd', 'rev'))
            if not rev.is_file():
                argp.error(f'reverse fastq file does not exist: {rev}')

            data.append({
                'sample_id': row['SampleID'],
                'sample_dir': sample_dir,
                'fwd_fastq': fwd.relative_to(root),
                'rev_fastq': rev.relative_to(root),
            })
    print('[OK]')

    return data


def get_sample_info():
    """ get sample info from filesystem """
    base = proj_dir / 'amplicons'
    absroot = root.resolve()
    if not base.is_dir():
        argp.error(f'no such directory: {base}')
    data = []
    for sampdir in base.glob('samp_*'):
        # resolve the symlink but stay relative to root
        sampdir = sampdir.resolve().relative_to(absroot)
        data.append({
            'sample_id': sampdir.name,
            'sample_dir': sampdir,
            'fwd_fastq': sampdir / 'reads' / 'raw_fwd_reads.fastq.gz',
            'rev_fastq': sampdir / 'reads' / 'raw_rev_reads.fastq.gz',
        })
    return data


def get_target_info_sample(sample_info):
    """
    Helper for get_target_info()

    Collect the target data for one sample.  Test mode is implemented here.
    """
    sample_id = sample_info['sample_id']
    detect_region_dir = root / sample_info['sample_dir'] / 'detect_region'
    if not detect_region_dir.is_dir():
        print(f'{sample_id}: not a directory: {detect_region_dir=}')
        return NO_TARGET_INFO

    if args.test_mode:
        # in test mode generate target info from nhmmscan output
        tmpd = None
        script_dir = Path(__file__).resolve().parent
        try:
            sum_fwd = detect_region_dir / 'fwd_summary.tsv'
            sum_rev = detect_region_dir / 'rev_summary.tsv'

            proc_hmm_fwd = proc_hmm_rev = None
            if not sum_fwd.is_file or not sum_rev.is_file():
                tmpd = TemporaryDirectory()
                tmp = Path(tmpd.name)
                hmm_fwd = detect_region_dir / 'fwd.txt'
                hmm_rev = detect_region_dir / 'rev.txt'
                sum_fwd = tmp / 'fwd_summary.tsv'
                sum_rev = tmp / 'rev_summary.tsv'
                sum_script = script_dir / SUMMARIZE_HMMSCAN_SCRIPT
                proc_hmm_fwd = run(
                    [sum_script, '--input', hmm_fwd, '--output', sum_fwd],
                    capture_output=True,
                    check=True,
                    cwd=tmp,
                )
                proc_hmm_rev = run(
                    [sum_script, '--input', hmm_rev, '--output', sum_rev],
                    capture_output=True,
                    check=True,
                    cwd=tmp,
                )

            guess_script = script_dir / GUESS_TARGET_SCRIPT
            proc_guess = run(
                [guess_script, str(sum_fwd), str(sum_rev)],
                capture_output=True,
                check=True,
                cwd=tmp if tmpd else None,
            )
        except CalledProcessError as e:
            msg = [f'called {e.cmd[0]} // returncode: {e.returncode}']
            msg.append(f'  cmd line: {e.cmd}')
            msg.append(f'  stdout:\n{e.stdout.decode()}')
            msg.append(f'  stderr:\n{e.stderr.decode()}')
            for i in (proc_hmm_fwd, proc_hmm_rev):
                if i is not None:
                    msg.append(
                        f'Also ran {i.args[0]}: returncode: {i.returncode}'
                    )
                    msg.append(f'  cmd line: {i.args}')
                    if i.returncode:
                        msg.append(f'  stdout:\n{i.stdout.decode()}')
                        msg.append(f'  stderr:\n{i.stderr.decode()}')
            if tmpd:
                msg.append(f'Temporary directory listing: {tmpd.name}')
                for i in tmp.iterdir():
                    msg.append(f'  {i}')

            msg.append(f'{sample_info=}')
            print('', *(f'  [EE] {i}' for i in msg), sep='\n', file=sys.stderr)
            argp.error(f'ERROR calling external script: {e}')
        finally:
            if tmpd:
                tmpd.cleanup()

        return proc_guess.stdout.decode()
    else:
        try:
            return (detect_region_dir / 'target_info.txt').read_text()
        except FileNotFoundError:
            return NO_TARGET_INFO


def get_target_info(data):
    """ Collect and add amplicon target information to data """

    info_pat = re.compile(r'^# \[INFO\] (?P<key>[^=]+)=(?P<value>.*)$')
    for sample_info in data:
        info_txt = get_target_info_sample(sample_info)
        if info_txt == NO_TARGET_INFO:
            sample_info['tax_group'] = NO_TARGET_INFO
            sample_info['gene'] = NO_TARGET_INFO
            sample_info['regions'] = NO_TARGET_INFO
        else:
            info_txt = info_txt.splitlines()
            try:
                header = info_txt[0]
            except KeyError:
                argp.error('target info txt is empty?')

            if info_txt[1].startswith('#'):
                # no hits, indicate target unknown
                sample_info['tax_group'] = ''
                sample_info['gene'] = ''
                sample_info['regions'] = ''
                sample_info['got_error'] = True
                rest = info_txt[1:]
            else:
                # second line is row with target guess
                items = zip(
                    header.rstrip('\n').split('\t'),
                    info_txt[1].rstrip('\n').split('\t'),
                    strict=True,
                )
                for key, value in items:
                    sample_info[key] = value
                rest = info_txt[2:]

            for line in rest:
                if '[ERROR]' in line:
                    # TODO: improve error reporting
                    sample_info['errors'] = True
                    break

            info = {}
            for line in rest:
                if m := info_pat.match(line):
                    info[m.group('key')] = m.group('value')
            sample_info['info'] = info


def dispatch(data):
    """
    Write target assignment file and sample file listing.

    These teo output files are meant to be consumed by the dada2 pipeline
    script.  A backup of an existing assignment file is preserved to allow to
    make comparisons.
    """
    # 1. assign target designation
    for sample_data in data:
        tax_group = sample_data['tax_group']
        gene = sample_data['gene']
        regions = sample_data['regions']

        if tax_group == gene == regions == '':
            # detection failed due to the reads not being similar enough to any
            # of the models
            target = UNKNOWN
        elif tax_group == gene == regions == NO_TARGET_INFO:
            # detection failed for technical reasons, e.g. intermediate
            # pipeline steps failed or relevant data missing
            target = NO_INFO
        else:
            target = f'{tax_group}-{gene}-{regions.replace(",", "-")}'

        sample_data['target'] = target

    stats = Counter((i['target'] for i in data))
    print('Target stats:',
          *(f' {v:>4} x {k}' for k, v in sorted(stats.items())),
          sep='\n')

    # 2. single mode detection
    real_targets = [i for i in stats if i not in (NO_INFO, UNKNOWN)]
    if len(real_targets) == 1:
        # single mode
        mode = real_targets[0]
        if stats[mode] / len(data) >= SINGLE_MODE_THRESHOLT:
            override_count = 0
            for sample_data in data:
                if sample_data['target'] != mode:
                    sample_data['override'] = mode
                    override_count += 1
            if override_count:
                print(f'[NOTICE] Single target mode detected - overriding '
                      f'{override_count} stray samples to {mode}')

    # 2. make backup of old file
    if (old := Path(args.out_targets)).is_file():
        mtime = datetime.fromtimestamp(old.stat().st_mtime).isoformat()
        backup = old.with_name(old.name + '.' + mtime)
        old.rename(backup)

    # 3. write target assignment file
    write_output(
        args.out_targets,
        data,
        columns=['sample_id', 'target', 'override'],
        order_by=('target', 'sample_id'),
    )

    # 4. write sample files listing
    write_output(
        args.out_samples,
        data,
        columns=[
            'sample_id', 'swapped', 'sample_dir', 'fwd_fastq', 'rev_fastq'
        ],
        order_by=('sample_id',),
    )


def write_output(file, data, columns, order_by=None):
    """ write an output file """
    def sortkey(row):
        key = []
        if order_by is None:
            return None
        for item in order_by:
            if order_by == 'sample_id':
                try:
                    snum = int(row['sample_id'].removeprefix('samp_'))
                except ValueError:
                    snum = row['sample_id']
                key.append(snum)
            else:
                key.append(row[item])
        return key

    with open(file, 'w') as ofile:
        ofile.write('\t'.join(columns))
        ofile.write('\n')
        for row in sorted(data, key=sortkey if order_by else None):
            row = [row.get(i) for i in columns]
            row = ['' if i is None else str(i) for i in row]
            ofile.write('\t'.join(row))
            ofile.write('\n')
    print(f'Wrote {len(data)} records to {file}')


# MAIN
data = get_sample_info_log() if args.use_import_log else get_sample_info()
if data:
    print(f'Found info on {len(data)} samples')
else:
    argp.error('No samples found')
print('Collecting amplicon target info... ', end='', flush=True)
get_target_info(data)
print(f'{len(data)}  [OK]')

dispatch(data)

if args.out_details:
    # TODO: get me more info bits from hmm summary
    rows = []
    cols = ['sample_id', 'error'] + [i for i in data[0]['info'].keys()]
    extra_cols = ['tax_group', 'gene', 'regions']
    for i in data:
        row = i.get('info', {})
        row['sample_id'] = i['sample_id']
        row['errors'] = '(EE)' if i.get('errors') else ''

        for k in extra_cols:
            row[k] = i[k]
        rows.append(row)

    write_output(
        args.out_details,
        rows,
        columns=cols + extra_cols,
        order_by=('tax_group', 'gene', 'regions', 'sample_id'),
    )
