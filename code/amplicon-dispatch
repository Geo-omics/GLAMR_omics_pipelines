#!/usr/bin/env python3
"""
Run DADA2 on amplicon sequences of given dataset to obtain ASVs and abundance
"""
import argparse
from datetime import datetime
from pathlib import Path
from subprocess import CalledProcessError, run
import sys
from tempfile import TemporaryDirectory


GUESS_TARGET_SCRIPT = './guess-amplicon-target'
""" helper script, path relative to this script """

SUMMARIZE_HMMSCAN_SCRIPT = './summarize_hmmscan.R'
""" helper script, path relative to this script """


argp = argparse.ArgumentParser(description=__doc__)
argp.add_argument(
    'project_directory',
    help='The project directory, usually data/projects/<dataset> where the '
         'last component of the given path is interpreted as the dataset ID.  '
         'This directory is expected to contain an "amplicon" subdirectory '
         'that in turn contains one subdirectory per sample.',
)
argp.add_argument(
    '--test-mode',
    action='store_true',
    help='Run script in test mode.  In test mode...',
)
argp.add_argument(
    '--use-import-log',
    action='store_true',
    help='Get info on a dataset\'s samples from the import log.  This also '
    'checks that raw read files, etc. actually exist.  The default is to just '
    'look at the filesystem and does not check for file existence.  The '
    'snakemake run amplicon pipeline should use the default.',
)
argp.add_argument(
    '--out-targets',
    default='./amplicon_target_assignments.tsv',
    help='Path to output sample listing file.  This is a tab-separated table '
         'listing samples with amplicon targets and paths to raw reads.',
)
argp.add_argument(
    '--out-samples',
    default='./sample_files.tsv',
    help='Path to output sample listing file.  This is a tab-separated table '
         'listing samples with amplicon targets and paths to raw reads.',
)
argp.add_argument(
    '--out-details',
    default=None,
    help='Path to output detailed amplicon target information file.'
)
args = argp.parse_args()


# assuming proj dir is like data/project/set_xx and data/import_logs exists
proj_dir = Path(args.project_directory)
if not proj_dir.is_dir():
    argp.error(f'no such directory: {proj_dir}')
if proj_dir.parts[-3:-1] != ('data', 'projects'):
    # not like path/to/data/projects/set_nn
    print('[WARNING] non-standard project directory', file=sys.stderr)
root = proj_dir.parent.parent.parent  # the GLAMR root directory


NO_TARGET_INFO = object()


def get_sample_info_log():
    """ collect info from omics pipeline import log """
    import_log_dir = Path(args.project_directory).parent.parent / 'import_logs'
    try:
        import_log = sorted(import_log_dir.glob('*_sample_status.tsv'))[-1]
    except Exception as e:
        argp.error(f'Failed finding import log: {e.__class__.__name__}: {e}')

    data = []
    with open(import_log) as ifile:
        print(f'Reading import log: {import_log}... ', end='', flush=True)
        header = ifile.readline().strip().split('\t')
        for line in ifile:
            row = dict(zip(header, line.strip().split('\t')))

            if not row['StudyID'] == args.dataset:
                continue

            if not row['sample_type'] == 'amplicon':
                continue

            if not row['import_success'] == 'TRUE':
                continue

            # check sample dir
            try:
                sample_dir = Path(row['sample_dir'])
            except ValueError as e:
                argp.error(f'failed parsing sample_dir: {e}\n{row=}')

            # get paths to fastq files
            fwd = Path(row['raw_reads_fp'])
            try:
                fwd = root / fwd
            except ValueError as e:
                argp.error(f'unexpected path: {e}\n{row=}')

            if not fwd.is_file():
                argp.error(f'forward fastq file does not exist: {fwd}')

            rev = fwd.parent / (fwd.name.replace('fwd', 'rev'))
            if not rev.is_file():
                argp.error(f'reverse fastq file does not exist: {rev}')

            data.append({
                'sample_id': row['SampleID'],
                'sample_dir': sample_dir,
                'fwd_fastq': fwd.relative_to(root),
                'rev_fastq': rev.relative_to(root),
            })
    print('[OK]')

    return data


def get_sample_info():
    """ get sample info from filesystem """
    base = proj_dir / 'amplicons'
    absroot = root.resolve()
    if not base.is_dir():
        argp.error(f'no such directory: {base}')
    data = []
    for sampdir in base.glob('samp_*'):
        # resolve the symlink but stay relative to root
        sampdir = sampdir.resolve().relative_to(absroot)
        data.append({
            'sample_id': sampdir.name,
            'sample_dir': sampdir,
            'fwd_fastq': sampdir / 'reads' / 'raw_fwd_reads.fastq.gz',
            'rev_fastq': sampdir / 'reads' / 'raw_rev_reads.fastq.gz',
        })
    return data


def get_target_info_sample(sample_info):
    """ helper for get_target_info() """
    sample_id = sample_info['sample_id']
    detect_region_dir = root / sample_info['sample_dir'] / 'detect_region'
    if not detect_region_dir.is_dir():
        print(f'{sample_id}: not a directory: {detect_region_dir=}')
        return NO_TARGET_INFO

    if args.test_mode:
        # in test mode generate target info from nhmmscan output
        tmpd = None
        script_dir = Path(__file__).resolve().parent
        try:
            sum_fwd = detect_region_dir / 'fwd_summary.tsv'
            sum_rev = detect_region_dir / 'rev_summary.tsv'

            if not sum_fwd.is_file or not sum_rev.is_file():
                tmpd = TemporaryDirectory()
                tmp = Path(tmpd.name)
                hmm_fwd = detect_region_dir / 'fwd.txt'
                hmm_rev = detect_region_dir / 'rev.txt'
                sum_fwd = tmp / 'fwd_summary.tsv'
                sum_rev = tmp / 'rev_summary.tsv'
                sum_script = script_dir / SUMMARIZE_HMMSCAN_SCRIPT
                run(
                    [sum_script, '--input', hmm_fwd, '--output', sum_fwd],
                    capture_output=True,
                    check=True,
                    cwd=tmp,
                )
                run(
                    [sum_script, '--input', hmm_rev, '--output', sum_fwd],
                    capture_output=True,
                    check=True,
                    cwd=tmp,
                )

            guess_script = script_dir / GUESS_TARGET_SCRIPT
            proc = run(
                [guess_script, str(sum_fwd), str(sum_rev)],
                capture_output=True,
                check=True,
                cwd=tmp if tmpd else None,
            )
        except CalledProcessError as e:
            print(e.stderr.decode(), file=sys.stderr)
            print(f'{sample_info=}', file=sys.stderr)
            argp.error(f'ERROR calling {e}')
        finally:
            if tmpd:
                tmpd.cleanup()

        return proc.stdout.decode()
    else:
        try:
            return (detect_region_dir / 'target_info.txt').read_text()
        except FileNotFoundError:
            return NO_TARGET_INFO


def get_target_info(data):
    """ detect amplicon target genes and regions """

    for sample_info in data:
        info_txt = get_target_info_sample(sample_info)
        if info_txt == NO_TARGET_INFO:
            sample_info['tax_group'] = NO_TARGET_INFO
            sample_info['gene'] = NO_TARGET_INFO
            sample_info['regions'] = NO_TARGET_INFO
        else:
            try:
                header, row, *errors = info_txt.splitlines()
            except Exception as e:
                argp.error(
                    f'expected two lines of target info txt: {e}'
                )

            target_data = dict(zip(
                header.rstrip('\n').split('\t'),
                row.rstrip('\n').split('\t'),
                strict=True,
            ))
            cols = ['tax_group', 'gene', 'regions', 'dirs_swapped',
                    'got_error']
            for key in cols:
                sample_info[key] = target_data[key]


def dispatch(data):
    """
    Write target assignemnt file and sample file listing.

    These are the two inputs, to be consumed by the dada2 pipeline script.  A
    backup of an existing assignment file is preserved to allow to make
    comparisons.
    """
    # 1. assign target designation
    for sample_data in data:
        tax_group = sample_data['tax_group']
        gene = sample_data['gene']
        regions = sample_data['regions']

        if tax_group == gene == regions == '':
            # detection failed due to the reads not being similar enough to any
            # of the models
            target = 'UNKNOWN'
        elif tax_group == gene == regions == NO_TARGET_INFO:
            # detection failed for technical reasons, e.g. intermediate
            # pipeline steps failed
            target = 'NO_INFO'
        else:
            target = f'{tax_group}-{gene}-{regions.replace(",", "-")}'

        sample_data['target'] = target

    # 2. make backup of old file
    if (old := Path(args.out_targets)).is_file():
        mtime = datetime.fromtimestamp(old.stat().st_mtime).isoformat()
        backup = old.with_name(old.name + '.' + mtime)
        old.rename(backup)

    # 3. write target assignment file
    write_output(
        args.out_targets,
        data,
        columns=['sample_id', 'target', 'override'],
        order_by=('target', 'sample_id'),
    )

    # 4. write sample files listing
    write_output(
        args.out_samples,
        data,
        columns=[
            'sample_id', 'swapped', 'sample_dir', 'fwd_fastq', 'rev_fastq'
        ],
        order_by=('sample_id',),
    )


def write_output(file, data, columns, order_by=None):
    """ write an output file """
    def sortkey(row):
        key = []
        if order_by is None:
            return None
        for item in order_by:
            if order_by == 'sample_id':
                try:
                    snum = int(row['sample_id'].removeprefix('samp_'))
                except ValueError:
                    snum = row['sample_id']
                key.append(snum)
            else:
                key.append(row[item])
        return key

    with open(file, 'w') as ofile:
        ofile.write('\t'.join(columns))
        ofile.write('\n')
        for row in sorted(data, key=sortkey if order_by else None):
            row = [row.get(i) for i in columns]
            row = ['' if i is None else str(i) for i in row]
            ofile.write('\t'.join(row))
            ofile.write('\n')
    print(f'Wrote {len(data)} records to {file}')


# MAIN
data = get_sample_info_log() if args.use_import_log else get_sample_info()
if data:
    print(f'Found info on {len(data)} samples')
else:
    argp.error('No samples found')
print('Collecting amplicon target info... ', end='', flush=True)
get_target_info(data)
print(f'{len(data)}  [OK]')
if args.out_details:
    # TODO: get me more info bits from hmm summary
    write_output(
        args.out_details,
        data,
        columns=['sample_id', 'dirs_swapped', 'tax_group', 'gene', 'regions'],
        order_by=('tax_group', 'gene', 'regions', 'sample_id'),
    )
dispatch(data)
